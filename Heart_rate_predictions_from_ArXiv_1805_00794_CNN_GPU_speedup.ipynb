{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Heart_rate_predictions_from_ArXiv_1805_00794_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeepaamcp/ECG_to_heart_rate_derivation/blob/dev/Heart_rate_predictions_from_ArXiv_1805_00794_CNN_GPU_speedup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "fU1WKbabPItn",
        "colab_type": "code",
        "outputId": "0f69c2d1-702a-405c-ef3b-78764fade827",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# THE MAIN SOURCE FOR THIS CODE IS https://www.kaggle.com/coni57/model-from-arxiv-1805-00794\n",
        "# THE ASSOCIATED RESEARCH PAPER : https://arxiv.org/pdf/1805.00794\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "import itertools\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, label_ranking_average_precision_score, label_ranking_loss, coverage_error \n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from scipy.signal import resample\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "import pickle\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Softmax, Add, Flatten, Activation# , Dropout\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "EJJfnMbxPItu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/datasets/test_results/csv/values.csv', header=None)\n",
        "# df2 = pd.read_csv(\"../input/mitbih_test.csv\", header=None)\n",
        "# df = pd.concat([df, df2], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "494fc8a26ba40beb73fc1a4f7b219b213fb7705e",
        "id": "GkNmHr8cPIty",
        "colab_type": "code",
        "outputId": "a9d3f0f8-e276-4e60-d2ec-3beeba4e45ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>105.820</td>\n",
              "      <td>106.762</td>\n",
              "      <td>106.007</td>\n",
              "      <td>106.007</td>\n",
              "      <td>105.079</td>\n",
              "      <td>105.820</td>\n",
              "      <td>105.820</td>\n",
              "      <td>104.895</td>\n",
              "      <td>105.634</td>\n",
              "      <td>106.007</td>\n",
              "      <td>104.895</td>\n",
              "      <td>105.263</td>\n",
              "      <td>104.530</td>\n",
              "      <td>105.448</td>\n",
              "      <td>105.634</td>\n",
              "      <td>104.895</td>\n",
              "      <td>105.448</td>\n",
              "      <td>105.634</td>\n",
              "      <td>105.263</td>\n",
              "      <td>105.448</td>\n",
              "      <td>104.712</td>\n",
              "      <td>105.448</td>\n",
              "      <td>106.007</td>\n",
              "      <td>105.448</td>\n",
              "      <td>105.634</td>\n",
              "      <td>106.007</td>\n",
              "      <td>105.448</td>\n",
              "      <td>105.634</td>\n",
              "      <td>104.895</td>\n",
              "      <td>105.634</td>\n",
              "      <td>105.634</td>\n",
              "      <td>105.634</td>\n",
              "      <td>105.820</td>\n",
              "      <td>106.195</td>\n",
              "      <td>105.079</td>\n",
              "      <td>105.820</td>\n",
              "      <td>105.263</td>\n",
              "      <td>106.007</td>\n",
              "      <td>106.195</td>\n",
              "      <td>105.634</td>\n",
              "      <td>...</td>\n",
              "      <td>106.007</td>\n",
              "      <td>106.195</td>\n",
              "      <td>106.007</td>\n",
              "      <td>105.448</td>\n",
              "      <td>106.007</td>\n",
              "      <td>105.079</td>\n",
              "      <td>105.634</td>\n",
              "      <td>105.448</td>\n",
              "      <td>106.383</td>\n",
              "      <td>105.634</td>\n",
              "      <td>105.079</td>\n",
              "      <td>106.383</td>\n",
              "      <td>105.263</td>\n",
              "      <td>105.263</td>\n",
              "      <td>106.007</td>\n",
              "      <td>105.079</td>\n",
              "      <td>105.263</td>\n",
              "      <td>105.634</td>\n",
              "      <td>104.712</td>\n",
              "      <td>105.079</td>\n",
              "      <td>105.448</td>\n",
              "      <td>103.986</td>\n",
              "      <td>105.263</td>\n",
              "      <td>105.448</td>\n",
              "      <td>104.712</td>\n",
              "      <td>105.079</td>\n",
              "      <td>105.634</td>\n",
              "      <td>105.079</td>\n",
              "      <td>105.634</td>\n",
              "      <td>104.530</td>\n",
              "      <td>105.263</td>\n",
              "      <td>105.634</td>\n",
              "      <td>105.079</td>\n",
              "      <td>105.634</td>\n",
              "      <td>104.895</td>\n",
              "      <td>105.079</td>\n",
              "      <td>104.895</td>\n",
              "      <td>105.079</td>\n",
              "      <td>105.448</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>65.005</td>\n",
              "      <td>65.076</td>\n",
              "      <td>63.694</td>\n",
              "      <td>67.039</td>\n",
              "      <td>66.815</td>\n",
              "      <td>66.298</td>\n",
              "      <td>60.423</td>\n",
              "      <td>59.880</td>\n",
              "      <td>60.302</td>\n",
              "      <td>66.741</td>\n",
              "      <td>70.588</td>\n",
              "      <td>69.606</td>\n",
              "      <td>65.359</td>\n",
              "      <td>62.048</td>\n",
              "      <td>66.964</td>\n",
              "      <td>66.890</td>\n",
              "      <td>62.565</td>\n",
              "      <td>60.914</td>\n",
              "      <td>62.500</td>\n",
              "      <td>66.152</td>\n",
              "      <td>70.671</td>\n",
              "      <td>70.922</td>\n",
              "      <td>67.568</td>\n",
              "      <td>63.966</td>\n",
              "      <td>69.444</td>\n",
              "      <td>71.942</td>\n",
              "      <td>71.856</td>\n",
              "      <td>67.492</td>\n",
              "      <td>66.815</td>\n",
              "      <td>66.593</td>\n",
              "      <td>69.767</td>\n",
              "      <td>67.720</td>\n",
              "      <td>66.890</td>\n",
              "      <td>67.720</td>\n",
              "      <td>70.671</td>\n",
              "      <td>70.175</td>\n",
              "      <td>68.571</td>\n",
              "      <td>66.519</td>\n",
              "      <td>68.807</td>\n",
              "      <td>70.671</td>\n",
              "      <td>...</td>\n",
              "      <td>65.502</td>\n",
              "      <td>68.650</td>\n",
              "      <td>70.505</td>\n",
              "      <td>67.873</td>\n",
              "      <td>62.565</td>\n",
              "      <td>63.358</td>\n",
              "      <td>66.079</td>\n",
              "      <td>70.258</td>\n",
              "      <td>67.340</td>\n",
              "      <td>64.935</td>\n",
              "      <td>64.655</td>\n",
              "      <td>67.189</td>\n",
              "      <td>67.644</td>\n",
              "      <td>66.007</td>\n",
              "      <td>65.646</td>\n",
              "      <td>70.755</td>\n",
              "      <td>70.922</td>\n",
              "      <td>67.492</td>\n",
              "      <td>65.717</td>\n",
              "      <td>66.890</td>\n",
              "      <td>68.027</td>\n",
              "      <td>66.298</td>\n",
              "      <td>65.359</td>\n",
              "      <td>65.717</td>\n",
              "      <td>73.171</td>\n",
              "      <td>73.260</td>\n",
              "      <td>71.429</td>\n",
              "      <td>70.755</td>\n",
              "      <td>71.770</td>\n",
              "      <td>71.006</td>\n",
              "      <td>65.076</td>\n",
              "      <td>66.007</td>\n",
              "      <td>70.340</td>\n",
              "      <td>72.202</td>\n",
              "      <td>72.551</td>\n",
              "      <td>71.344</td>\n",
              "      <td>73.983</td>\n",
              "      <td>71.259</td>\n",
              "      <td>66.298</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>80.214</td>\n",
              "      <td>81.411</td>\n",
              "      <td>82.531</td>\n",
              "      <td>81.522</td>\n",
              "      <td>81.411</td>\n",
              "      <td>79.787</td>\n",
              "      <td>82.079</td>\n",
              "      <td>81.744</td>\n",
              "      <td>80.645</td>\n",
              "      <td>80.863</td>\n",
              "      <td>82.192</td>\n",
              "      <td>82.531</td>\n",
              "      <td>80.645</td>\n",
              "      <td>80.537</td>\n",
              "      <td>82.418</td>\n",
              "      <td>81.081</td>\n",
              "      <td>79.576</td>\n",
              "      <td>81.744</td>\n",
              "      <td>81.855</td>\n",
              "      <td>81.633</td>\n",
              "      <td>79.787</td>\n",
              "      <td>81.967</td>\n",
              "      <td>83.102</td>\n",
              "      <td>79.893</td>\n",
              "      <td>78.227</td>\n",
              "      <td>80.537</td>\n",
              "      <td>79.893</td>\n",
              "      <td>80.107</td>\n",
              "      <td>81.633</td>\n",
              "      <td>80.321</td>\n",
              "      <td>81.522</td>\n",
              "      <td>82.988</td>\n",
              "      <td>78.947</td>\n",
              "      <td>80.107</td>\n",
              "      <td>80.972</td>\n",
              "      <td>79.681</td>\n",
              "      <td>79.893</td>\n",
              "      <td>81.744</td>\n",
              "      <td>81.191</td>\n",
              "      <td>80.863</td>\n",
              "      <td>...</td>\n",
              "      <td>78.947</td>\n",
              "      <td>80.537</td>\n",
              "      <td>80.214</td>\n",
              "      <td>79.470</td>\n",
              "      <td>81.081</td>\n",
              "      <td>83.218</td>\n",
              "      <td>77.922</td>\n",
              "      <td>80.214</td>\n",
              "      <td>79.787</td>\n",
              "      <td>78.844</td>\n",
              "      <td>78.844</td>\n",
              "      <td>81.081</td>\n",
              "      <td>80.214</td>\n",
              "      <td>79.365</td>\n",
              "      <td>83.333</td>\n",
              "      <td>81.633</td>\n",
              "      <td>79.681</td>\n",
              "      <td>80.000</td>\n",
              "      <td>78.329</td>\n",
              "      <td>77.922</td>\n",
              "      <td>80.863</td>\n",
              "      <td>79.787</td>\n",
              "      <td>79.260</td>\n",
              "      <td>80.972</td>\n",
              "      <td>79.787</td>\n",
              "      <td>79.787</td>\n",
              "      <td>81.855</td>\n",
              "      <td>78.947</td>\n",
              "      <td>80.214</td>\n",
              "      <td>78.947</td>\n",
              "      <td>78.740</td>\n",
              "      <td>80.754</td>\n",
              "      <td>79.681</td>\n",
              "      <td>80.000</td>\n",
              "      <td>81.522</td>\n",
              "      <td>80.645</td>\n",
              "      <td>79.681</td>\n",
              "      <td>81.855</td>\n",
              "      <td>81.081</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>81.633</td>\n",
              "      <td>82.418</td>\n",
              "      <td>62.176</td>\n",
              "      <td>106.007</td>\n",
              "      <td>80.537</td>\n",
              "      <td>64.171</td>\n",
              "      <td>82.873</td>\n",
              "      <td>79.156</td>\n",
              "      <td>77.821</td>\n",
              "      <td>96.463</td>\n",
              "      <td>118.812</td>\n",
              "      <td>67.039</td>\n",
              "      <td>94.937</td>\n",
              "      <td>61.350</td>\n",
              "      <td>102.041</td>\n",
              "      <td>122.449</td>\n",
              "      <td>95.087</td>\n",
              "      <td>60.729</td>\n",
              "      <td>78.844</td>\n",
              "      <td>85.349</td>\n",
              "      <td>80.429</td>\n",
              "      <td>81.967</td>\n",
              "      <td>100.000</td>\n",
              "      <td>105.263</td>\n",
              "      <td>92.166</td>\n",
              "      <td>65.217</td>\n",
              "      <td>91.047</td>\n",
              "      <td>73.439</td>\n",
              "      <td>81.191</td>\n",
              "      <td>81.301</td>\n",
              "      <td>83.916</td>\n",
              "      <td>82.192</td>\n",
              "      <td>78.125</td>\n",
              "      <td>73.892</td>\n",
              "      <td>79.156</td>\n",
              "      <td>77.922</td>\n",
              "      <td>86.207</td>\n",
              "      <td>81.744</td>\n",
              "      <td>81.855</td>\n",
              "      <td>84.151</td>\n",
              "      <td>...</td>\n",
              "      <td>78.534</td>\n",
              "      <td>80.537</td>\n",
              "      <td>79.365</td>\n",
              "      <td>78.637</td>\n",
              "      <td>81.411</td>\n",
              "      <td>80.972</td>\n",
              "      <td>77.519</td>\n",
              "      <td>80.972</td>\n",
              "      <td>83.333</td>\n",
              "      <td>82.873</td>\n",
              "      <td>81.633</td>\n",
              "      <td>81.301</td>\n",
              "      <td>81.191</td>\n",
              "      <td>77.821</td>\n",
              "      <td>76.142</td>\n",
              "      <td>83.218</td>\n",
              "      <td>83.916</td>\n",
              "      <td>84.270</td>\n",
              "      <td>83.218</td>\n",
              "      <td>83.682</td>\n",
              "      <td>79.893</td>\n",
              "      <td>73.260</td>\n",
              "      <td>77.620</td>\n",
              "      <td>78.740</td>\n",
              "      <td>79.156</td>\n",
              "      <td>80.972</td>\n",
              "      <td>84.626</td>\n",
              "      <td>83.565</td>\n",
              "      <td>79.893</td>\n",
              "      <td>78.023</td>\n",
              "      <td>82.873</td>\n",
              "      <td>81.855</td>\n",
              "      <td>79.681</td>\n",
              "      <td>82.079</td>\n",
              "      <td>84.034</td>\n",
              "      <td>83.565</td>\n",
              "      <td>82.759</td>\n",
              "      <td>85.227</td>\n",
              "      <td>82.873</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>160.428</td>\n",
              "      <td>162.162</td>\n",
              "      <td>162.602</td>\n",
              "      <td>160.428</td>\n",
              "      <td>161.290</td>\n",
              "      <td>158.730</td>\n",
              "      <td>158.730</td>\n",
              "      <td>160.858</td>\n",
              "      <td>161.290</td>\n",
              "      <td>160.858</td>\n",
              "      <td>160.428</td>\n",
              "      <td>161.290</td>\n",
              "      <td>158.730</td>\n",
              "      <td>157.480</td>\n",
              "      <td>161.290</td>\n",
              "      <td>160.858</td>\n",
              "      <td>159.574</td>\n",
              "      <td>158.311</td>\n",
              "      <td>160.428</td>\n",
              "      <td>160.858</td>\n",
              "      <td>161.725</td>\n",
              "      <td>156.658</td>\n",
              "      <td>156.250</td>\n",
              "      <td>157.068</td>\n",
              "      <td>158.311</td>\n",
              "      <td>158.311</td>\n",
              "      <td>155.844</td>\n",
              "      <td>155.844</td>\n",
              "      <td>155.440</td>\n",
              "      <td>91.884</td>\n",
              "      <td>151.899</td>\n",
              "      <td>158.730</td>\n",
              "      <td>152.672</td>\n",
              "      <td>149.254</td>\n",
              "      <td>153.453</td>\n",
              "      <td>150.754</td>\n",
              "      <td>148.515</td>\n",
              "      <td>157.480</td>\n",
              "      <td>156.250</td>\n",
              "      <td>85.592</td>\n",
              "      <td>...</td>\n",
              "      <td>158.311</td>\n",
              "      <td>159.151</td>\n",
              "      <td>160.000</td>\n",
              "      <td>160.858</td>\n",
              "      <td>160.858</td>\n",
              "      <td>160.428</td>\n",
              "      <td>157.480</td>\n",
              "      <td>96.000</td>\n",
              "      <td>84.746</td>\n",
              "      <td>81.191</td>\n",
              "      <td>80.645</td>\n",
              "      <td>80.863</td>\n",
              "      <td>80.107</td>\n",
              "      <td>79.576</td>\n",
              "      <td>78.844</td>\n",
              "      <td>79.787</td>\n",
              "      <td>78.431</td>\n",
              "      <td>136.986</td>\n",
              "      <td>136.054</td>\n",
              "      <td>83.682</td>\n",
              "      <td>134.529</td>\n",
              "      <td>134.228</td>\n",
              "      <td>83.102</td>\n",
              "      <td>126.582</td>\n",
              "      <td>86.580</td>\n",
              "      <td>131.868</td>\n",
              "      <td>85.349</td>\n",
              "      <td>77.320</td>\n",
              "      <td>77.419</td>\n",
              "      <td>78.329</td>\n",
              "      <td>78.431</td>\n",
              "      <td>78.534</td>\n",
              "      <td>86.455</td>\n",
              "      <td>77.519</td>\n",
              "      <td>85.227</td>\n",
              "      <td>79.365</td>\n",
              "      <td>79.787</td>\n",
              "      <td>78.125</td>\n",
              "      <td>82.192</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 102 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0        1        2        3    ...      98       99       100  101\n",
              "0  105.820  106.762  106.007  106.007  ...  104.895  105.079  105.448    0\n",
              "1   65.005   65.076   63.694   67.039  ...   73.983   71.259   66.298    8\n",
              "2   80.214   81.411   82.531   81.522  ...   79.681   81.855   81.081    0\n",
              "3   81.633   82.418   62.176  106.007  ...   82.759   85.227   82.873    8\n",
              "4  160.428  162.162  162.602  160.428  ...   79.787   78.125   82.192    0\n",
              "\n",
              "[5 rows x 102 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "5281cb19f54f3bd379f875c24ae52b3b15fcafaf",
        "id": "a4DIT7vNPIt1",
        "colab_type": "code",
        "outputId": "7b41d061-7de8-4a95-9962-5f509a6370b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 350 entries, 0 to 349\n",
            "Columns: 102 entries, 0 to 101\n",
            "dtypes: float64(101), int64(1)\n",
            "memory usage: 279.0 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "0fac0fb658ea34b48055838b4ad85078e883360d",
        "id": "EB6HussvPIt5",
        "colab_type": "code",
        "outputId": "b7d22fd0-30f6-4039-a46a-4c89f5d845d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "df[101].value_counts()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    230\n",
              "8     83\n",
              "2     15\n",
              "1     14\n",
              "6      4\n",
              "3      3\n",
              "5      1\n",
              "Name: 101, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "147b7604bd8a389d7f6aa111f38ae308af7c4eb7",
        "id": "Z-5hcbF_PIt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "M = df.values\n",
        "X = M[:, :-1]\n",
        "y = M[:, -1].astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n82ksHULfRWA",
        "colab_type": "code",
        "outputId": "9d517c4a-c335-4971-835c-59685e502707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "y"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 8, 0, 8, 0, 8, 0, 8, 0, 1, 0, 0, 8, 0, 8, 2, 8, 8, 8, 8, 1, 8,\n",
              "       8, 1, 8, 2, 8, 2, 2, 2, 0, 2, 0, 2, 0, 2, 5, 8, 2, 0, 0, 8, 8, 2,\n",
              "       2, 8, 8, 1, 8, 8, 0, 2, 0, 3, 1, 2, 8, 2, 8, 8, 8, 1, 8, 0, 8, 8,\n",
              "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 1, 8, 8, 8, 1, 8, 1, 8, 3,\n",
              "       8, 0, 8, 8, 8, 0, 8, 1, 6, 8, 8, 8, 8, 0, 6, 8, 0, 8, 6, 0, 6, 0,\n",
              "       0, 8, 0, 8, 8, 8, 8, 3, 8, 0, 8, 0, 8, 1, 8, 1, 8, 0, 8, 8, 0, 8,\n",
              "       0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0,\n",
              "       1, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 8, 0, 2, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "504d95532114efa4cc581d80bf02159c3ce519c6",
        "id": "HBPmVCNKPIuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# del df\n",
        "# del df2\n",
        "# del M"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "425c4b7abe39a14c6f81f8a71094cc1024276935",
        "id": "gAbFl72OPIuD",
        "colab_type": "text"
      },
      "source": [
        "# Visual Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "bcd502ecd1eb95bbf8af983396d3d0c3fb50ce4b",
        "id": "3daQE28mPIuF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "aa07c602-9f33-46a6-97f3-f2f48e72e215"
      },
      "source": [
        "C0 = np.argwhere(y == 0).flatten()\n",
        "C8 = np.argwhere(y == 8).flatten()\n",
        "\n",
        "print(C8)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  1   3   5   7  12  14  16  17  18  19  21  22  24  26  37  41  42  45\n",
            "  46  48  49  56  58  59  60  62  64  65  66  67  68  69  70  71  72  73\n",
            "  74  75  76  77  80  81  82  84  86  88  90  91  92  94  97  98  99 100\n",
            " 103 105 111 113 114 115 116 118 120 122 124 126 128 129 131 135 328 335\n",
            " 336 337 338 339 340 341 342 343 344 346 349]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rdjYeUkzUzw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "194aed4c-83a9-4a1c-a830-9bd957a29cf2"
      },
      "source": [
        "print(X[C0,:])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[105.82  106.762 106.007 ... 104.895 105.079 105.448]\n",
            " [ 80.214  81.411  82.531 ...  79.681  81.855  81.081]\n",
            " [160.428 162.162 162.602 ...  79.787  78.125  82.192]\n",
            " ...\n",
            " [ 70.588  71.344  71.006 ...  71.77   70.838  70.838]\n",
            " [ 70.093  69.606  69.045 ...  65.862  63.694  62.959]\n",
            " [ 60.241  60.241  59.88  ...  63.559  63.694  61.1  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "85209065f110cea77f4d65053d1164e9ee816049",
        "id": "FqYklUdTPIuI",
        "colab_type": "code",
        "outputId": "952f6f9a-e50f-401e-b9e1-37cbfb60a68f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "x = np.arange(0, 101)\n",
        "X[C8, :][0].shape\n",
        "plt.figure(figsize=(20,2))\n",
        "plt.plot(x, X[C0, :][0], label=\"Cat. N\")\n",
        "# plt.plot(x, X[C1, :][0], label=\"Cat. S\")\n",
        "# plt.plot(x, X[C2, :][0], label=\"Cat. V\")\n",
        "# plt.plot(x, X[C3, :][0], label=\"Cat. F\")\n",
        "# plt.plot(x, X[C4, :][0], label=\"Cat. Q\")\n",
        "plt.legend()\n",
        "plt.title(\"1-beat Heart Rate for category Normal\", fontsize=20)\n",
        "plt.ylabel(\"Amplitude\", fontsize=15)\n",
        "plt.xlabel(\"Time (s)\", fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJcAAAC0CAYAAAApfs1jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1440x144 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-en-x08Aymk2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "75b6e70b-fcae-47a3-cef8-7bffd72da80d"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[105.82  106.762 106.007 ... 104.895 105.079 105.448]\n",
            " [ 65.005  65.076  63.694 ...  73.983  71.259  66.298]\n",
            " [ 80.214  81.411  82.531 ...  79.681  81.855  81.081]\n",
            " ...\n",
            " [ 60.241  60.241  59.88  ...  63.559  63.694  61.1  ]\n",
            " [ 70.505  70.838  69.686 ...  68.182  66.593  65.717]\n",
            " [ 66.372  72.816  77.32  ...  61.538  62.762  70.755]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c42605d020fd51885437f4af3cf10cebbeafc9bb",
        "id": "foJsyFaTPIua",
        "colab_type": "text"
      },
      "source": [
        "# Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UJjD5as_YZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainMCI = np.random.choice(C0, 80)\n",
        "trainNorm = np.random.choice(C8, 80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkx6UgznBjiC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ea6fb582-2451-4843-edb3-5f284cefae7e"
      },
      "source": [
        "trainNorm"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  7,   3,  14, 100,   7, 100, 120,  45,  84,  71, 337, 118,  26,\n",
              "        91,  37, 335, 341, 118,  77, 343, 346,  99,  59,  62, 115,  80,\n",
              "        66,  37,  86, 124, 328,  18,   1,  17, 120,  21, 344,  17,  72,\n",
              "        72,  70,  12,  80,  65,  16, 336, 335,  22,  71,  70,  91,  58,\n",
              "       118,  74,  84,  72, 124,  90, 341,   5,   1,  12,  26,  64,  18,\n",
              "       342,  37,  81, 340,  97, 120,  98,   7,  58,  37,  82,  66,  73,\n",
              "        24,  69])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "c136b567ed0cf450ed0476464c3b59d1ff1bb032",
        "id": "79ReBm0DPIub",
        "colab_type": "code",
        "outputId": "cf030228-7f4e-4168-88d0-7c9bdff4b305",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "testMCI = np.random.choice(C0, 20)\n",
        "testNorm = np.random.choice(C8, 20)\n",
        "testNorm"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([118,   1,  64, 118, 340,   5, 135, 335,  64,  18, 118,  74,  97,\n",
              "        84,  59, 342, 114,  69,  98, 118])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QvECgau0Lep",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd1cec83-f636-4ead-89ac-317c6a4a8009"
      },
      "source": [
        "np.vstack([X[trainMCI], X[trainNorm]]).shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(160, 101)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SogjGTNmCOan",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2508761-fcfa-4b1a-a274-d9fcca52ccb1"
      },
      "source": [
        "np.hstack([y[trainMCI], y[trainNorm]]).shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(160,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K5zKxVG0dUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# np.hstack([y[subC0], y[subC1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "52573d0c3a715cd693e682227d01f5a73549e421",
        "id": "u_VqrWtBPIuh",
        "colab_type": "code",
        "outputId": "30326c02-2eff-459c-9f38-7bb1438adbc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "X_test = np.vstack([X[testMCI], X[testNorm]])\n",
        "y_test = np.hstack([y[testMCI], y[testNorm]])\n",
        "\n",
        "X_train = np.vstack([X[trainMCI], X[trainNorm]])\n",
        "y_train = np.hstack([y[trainMCI], y[trainNorm]])\n",
        "\n",
        "X_train, y_train = shuffle(X_train, y_train, random_state=0)\n",
        "X_test, y_test = shuffle(X_test, y_test, random_state=0)\n",
        "\n",
        "# del X\n",
        "# del y\n",
        "\n",
        "print(X_test.shape)\n",
        "print(X_train.shape)\n",
        "print(y_test.shape)\n",
        "print(y_train.shape)\n",
        "# print(X.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40, 101)\n",
            "(160, 101)\n",
            "(40,)\n",
            "(160,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKYLXEtUDGNK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7424bdfc-a658-4d2e-a3f3-1f3237e53b16"
      },
      "source": [
        "y_test"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([8, 8, 8, 0, 0, 0, 8, 0, 0, 8, 8, 8, 8, 0, 8, 8, 8, 0, 8, 0, 0, 0,\n",
              "       0, 0, 8, 0, 8, 0, 8, 0, 8, 8, 0, 8, 8, 0, 0, 8, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx0UcPr11Ib7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test = np.where(y_test > 1, 1, 0)\n",
        "y_train = np.where(y_train > 1, 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "7ae5108c9741b85f0f599cce51daf99df4733ed1",
        "id": "-yWqrtz6PIuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.expand_dims(X_train, 2)\n",
        "X_test = np.expand_dims(X_test, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "cbd350e57b2a44b4bf6a79f02c32dabb803e4855",
        "id": "nswRqohNPIuo",
        "colab_type": "code",
        "outputId": "57a89a27-0074-475c-a715-4cccdb43a562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"X_train\", X_train.shape)\n",
        "print(\"y_train\", y_train.shape)\n",
        "print(\"X_test\", X_test.shape)\n",
        "print(\"y_test\", y_test.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train (160, 101, 1)\n",
            "y_train (160,)\n",
            "X_test (40, 101, 1)\n",
            "y_test (40,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "5ddf1e7b397de3c413fc991945d2d7f09df67da1",
        "id": "CmHmd3RmPIur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ohe = OneHotEncoder()\n",
        "y_train = ohe.fit_transform(y_train.reshape(-1,1))\n",
        "y_test = ohe.transform(y_test.reshape(-1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "16c106c2702045790367fc49d7223560fc613d75",
        "id": "6jil13ZiPIuv",
        "colab_type": "code",
        "outputId": "b5b07bde-5160-47ef-c55a-3205e7ff7f84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"X_train\", X_train.shape)\n",
        "print(\"y_train\", y_train.shape)\n",
        "print(\"X_test\", X_test.shape)\n",
        "print(\"y_test\", y_test.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train (160, 101, 1)\n",
            "y_train (160, 2)\n",
            "X_test (40, 101, 1)\n",
            "y_test (40, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c4de23b85abe34a726eab268171da0e827bafa35",
        "id": "erKYAY4bPIuy",
        "colab_type": "text"
      },
      "source": [
        "# Model\n",
        "\n",
        "Now let's re-create the model from the ArXiv Document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40r5Acw8YsN5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "b41d1b73-2a77-4243-d82e-1e48db463d52"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 3927909748316409953\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 229599218253602439\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 7924150248523135813\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14912199066\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 4973011734883618283\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1wT6sENaMaC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed3a77ae-a629-4c6d-c467-023f1fc9a559"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import time\n",
        "\n",
        "os.cpu_count()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "fb0dc9775ddfa761c0ad948d59020fcbd2681c57",
        "id": "DrU2UcHvPIu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_obs, feature, depth = X_train.shape\n",
        "batch_size = 80"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTvXN7HefhqH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "f2e48ef8-e5d5-434a-b86b-9d1f2e1b14a5"
      },
      "source": [
        "#SERIAL VERSION\n",
        "K.clear_session()\n",
        "config = tf.ConfigProto( device_count = {'GPU': 0 , 'CPU': 1} ) \n",
        "sess = tf.Session(config=config) \n",
        "keras.backend.set_session(sess)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "e70fab0b07290e042ba9cd7c6cba37462a457b03",
        "id": "W8xgao8gPIu6",
        "colab_type": "code",
        "outputId": "af8935c1-6267-4592-f245-fb2f500dbcf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "inp = Input(shape=(feature, depth))\n",
        "C = Conv1D(filters=32, kernel_size=5, strides=1)(inp)\n",
        "\n",
        "C11 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(C)\n",
        "A11 = Activation(\"relu\")(C11)\n",
        "C12 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A11)\n",
        "S11 = Add()([C12, C])\n",
        "A12 = Activation(\"relu\")(S11)\n",
        "M11 = MaxPooling1D(pool_size=5, strides=2)(A12)\n",
        "\n",
        "\n",
        "C21 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M11)\n",
        "A21 = Activation(\"relu\")(C21)\n",
        "C22 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A21)\n",
        "S21 = Add()([C22, M11])\n",
        "A22 = Activation(\"relu\")(S11)\n",
        "M21 = MaxPooling1D(pool_size=5, strides=2)(A22)\n",
        "\n",
        "\n",
        "C31 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M21)\n",
        "A31 = Activation(\"relu\")(C31)\n",
        "C32 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A31)\n",
        "S31 = Add()([C32, M21])\n",
        "A32 = Activation(\"relu\")(S31)\n",
        "M31 = MaxPooling1D(pool_size=5, strides=2)(A32)\n",
        "\n",
        "\n",
        "C41 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M31)\n",
        "A41 = Activation(\"relu\")(C41)\n",
        "C42 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A41)\n",
        "S41 = Add()([C42, M31])\n",
        "A42 = Activation(\"relu\")(S41)\n",
        "M41 = MaxPooling1D(pool_size=5, strides=2)(A42)\n",
        "\n",
        "\n",
        "C51 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M41)\n",
        "A51 = Activation(\"relu\")(C51)\n",
        "C52 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A51)\n",
        "S51 = Add()([C52, M41])\n",
        "A52 = Activation(\"relu\")(S51)\n",
        "M51 = MaxPooling1D(pool_size=5, strides=2)(A52)\n",
        "\n",
        "F1 = Flatten()(M51)\n",
        "\n",
        "D1 = Dense(32)(F1)\n",
        "A6 = Activation(\"relu\")(D1)\n",
        "D2 = Dense(32)(A6)\n",
        "D3 = Dense(2)(D2)\n",
        "A7 = Softmax()(D3)\n",
        "\n",
        "model = Model(inputs=inp, outputs=A7)\n",
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 101, 1)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 97, 32)       192         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 97, 32)       5152        conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 97, 32)       0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 97, 32)       5152        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 97, 32)       0           conv1d_3[0][0]                   \n",
            "                                                                 conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 97, 32)       0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 47, 32)       0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 47, 32)       5152        max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 47, 32)       0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 47, 32)       5152        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 47, 32)       0           conv1d_7[0][0]                   \n",
            "                                                                 max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 47, 32)       0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 22, 32)       0           activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 22, 32)       5152        max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 22, 32)       0           conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_9 (Conv1D)               (None, 22, 32)       5152        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 22, 32)       0           conv1d_9[0][0]                   \n",
            "                                                                 max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 22, 32)       0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 9, 32)        0           activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 9, 32)        5152        max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 9, 32)        0           conv1d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_11 (Conv1D)              (None, 9, 32)        5152        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 9, 32)        0           conv1d_11[0][0]                  \n",
            "                                                                 max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 9, 32)        0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1D)  (None, 3, 32)        0           activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 96)           0           max_pooling1d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 32)           3104        flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32)           0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 32)           1056        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 2)            66          dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "softmax_1 (Softmax)             (None, 2)            0           dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 45,634\n",
            "Trainable params: 45,634\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "fdc0d8aa8475330af8d8e652d0b9ce214da66956",
        "id": "lazwYvDcPIu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def exp_decay(epoch):\n",
        "    initial_lrate = 0.002\n",
        "    k = 0.75\n",
        "    t = n_obs//(batch_size)  # every epoch we do n_obs/batch_size iteration\n",
        "    lrate = initial_lrate * math.exp(-k*t)\n",
        "    return lrate\n",
        "\n",
        "lrate = LearningRateScheduler(exp_decay)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "abcd2f0e8488c8f3b33cd6ed9ca7fd60fa44404b",
        "id": "Jom7PrMmPIvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adam = Adam(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "812e637ae56f6a4be9c8e98d3b501cfb11ef78cb",
        "id": "Kwc24PDmPIvH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "4a326c31-f77f-4f19-9dd0-c7a351bbcb40"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=adam,  metrics=['accuracy'])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "1e6ef643f6a55ab5b3c1b46126832c3ac45a6a2b",
        "id": "www7l_uDPIvL",
        "colab_type": "code",
        "outputId": "d83e22e0-4f38-458b-b974-c68571983026",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# It is observed that sometimes the accuracy won't change for epochs. \n",
        "# This is likely to happen due to Adam optimizer found a local minima\n",
        "# Source : https://stackoverflow.com/questions/37213388/keras-accuracy-does-not-change/53397560#53397560\n",
        "import time\n",
        "start_time = time.time()\n",
        "history = model.fit(X_train, y_train, \n",
        "                    epochs=300, \n",
        "                    batch_size=batch_size, \n",
        "                    verbose=1, \n",
        "                    validation_data=(X_test, y_test), \n",
        "                    callbacks=[lrate])\n",
        "\n",
        "elapsed_time_serial = time.time() - start_time\n",
        "print(\"Elapsed Time:\",elapsed_time_serial)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 160 samples, validate on 40 samples\n",
            "Epoch 1/300\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "160/160 [==============================] - 1s 5ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 2/300\n",
            "160/160 [==============================] - 0s 649us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 3/300\n",
            "160/160 [==============================] - 0s 666us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 4/300\n",
            "160/160 [==============================] - 0s 611us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 5/300\n",
            "160/160 [==============================] - 0s 590us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 6/300\n",
            "160/160 [==============================] - 0s 612us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 7/300\n",
            "160/160 [==============================] - 0s 604us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 8/300\n",
            "160/160 [==============================] - 0s 598us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 9/300\n",
            "160/160 [==============================] - 0s 644us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 10/300\n",
            "160/160 [==============================] - 0s 599us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 11/300\n",
            "160/160 [==============================] - 0s 619us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 12/300\n",
            "160/160 [==============================] - 0s 589us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 13/300\n",
            "160/160 [==============================] - 0s 605us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 14/300\n",
            "160/160 [==============================] - 0s 610us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 15/300\n",
            "160/160 [==============================] - 0s 685us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 16/300\n",
            "160/160 [==============================] - 0s 655us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 17/300\n",
            "160/160 [==============================] - 0s 585us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 18/300\n",
            "160/160 [==============================] - 0s 652us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 19/300\n",
            "160/160 [==============================] - 0s 627us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 20/300\n",
            "160/160 [==============================] - 0s 750us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 21/300\n",
            "160/160 [==============================] - 0s 648us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 22/300\n",
            "160/160 [==============================] - 0s 621us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 23/300\n",
            "160/160 [==============================] - 0s 595us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 24/300\n",
            "160/160 [==============================] - 0s 697us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 25/300\n",
            "160/160 [==============================] - 0s 578us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 26/300\n",
            "160/160 [==============================] - 0s 571us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 27/300\n",
            "160/160 [==============================] - 0s 560us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 28/300\n",
            "160/160 [==============================] - 0s 575us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 29/300\n",
            "160/160 [==============================] - 0s 617us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 30/300\n",
            "160/160 [==============================] - 0s 589us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 31/300\n",
            "160/160 [==============================] - 0s 601us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 32/300\n",
            "160/160 [==============================] - 0s 670us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 33/300\n",
            "160/160 [==============================] - 0s 609us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 34/300\n",
            "160/160 [==============================] - 0s 616us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 35/300\n",
            "160/160 [==============================] - 0s 693us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 36/300\n",
            "160/160 [==============================] - 0s 572us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 37/300\n",
            "160/160 [==============================] - 0s 601us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 38/300\n",
            "160/160 [==============================] - 0s 614us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 39/300\n",
            "160/160 [==============================] - 0s 632us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 40/300\n",
            "160/160 [==============================] - 0s 630us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 41/300\n",
            "160/160 [==============================] - 0s 607us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 42/300\n",
            "160/160 [==============================] - 0s 602us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 43/300\n",
            "160/160 [==============================] - 0s 650us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 44/300\n",
            "160/160 [==============================] - 0s 696us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 45/300\n",
            "160/160 [==============================] - 0s 669us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 46/300\n",
            "160/160 [==============================] - 0s 660us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 47/300\n",
            "160/160 [==============================] - 0s 608us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 48/300\n",
            "160/160 [==============================] - 0s 644us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 49/300\n",
            "160/160 [==============================] - 0s 618us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 50/300\n",
            "160/160 [==============================] - 0s 629us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 51/300\n",
            "160/160 [==============================] - 0s 639us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 52/300\n",
            "160/160 [==============================] - 0s 630us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 53/300\n",
            "160/160 [==============================] - 0s 598us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 54/300\n",
            "160/160 [==============================] - 0s 627us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 55/300\n",
            "160/160 [==============================] - 0s 642us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 56/300\n",
            "160/160 [==============================] - 0s 586us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 57/300\n",
            "160/160 [==============================] - 0s 582us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 58/300\n",
            "160/160 [==============================] - 0s 585us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 59/300\n",
            "160/160 [==============================] - 0s 631us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 60/300\n",
            "160/160 [==============================] - 0s 619us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 61/300\n",
            "160/160 [==============================] - 0s 610us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 62/300\n",
            "160/160 [==============================] - 0s 619us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 63/300\n",
            "160/160 [==============================] - 0s 658us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 64/300\n",
            "160/160 [==============================] - 0s 614us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 65/300\n",
            "160/160 [==============================] - 0s 723us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 66/300\n",
            "160/160 [==============================] - 0s 618us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 67/300\n",
            "160/160 [==============================] - 0s 590us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 68/300\n",
            "160/160 [==============================] - 0s 623us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 69/300\n",
            "160/160 [==============================] - 0s 663us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 70/300\n",
            "160/160 [==============================] - 0s 626us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 71/300\n",
            "160/160 [==============================] - 0s 626us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 72/300\n",
            "160/160 [==============================] - 0s 589us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 73/300\n",
            "160/160 [==============================] - 0s 635us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 74/300\n",
            "160/160 [==============================] - 0s 655us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 75/300\n",
            "160/160 [==============================] - 0s 698us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 76/300\n",
            "160/160 [==============================] - 0s 611us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 77/300\n",
            "160/160 [==============================] - 0s 575us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 78/300\n",
            "160/160 [==============================] - 0s 635us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 79/300\n",
            "160/160 [==============================] - 0s 594us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 80/300\n",
            "160/160 [==============================] - 0s 624us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 81/300\n",
            "160/160 [==============================] - 0s 641us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 82/300\n",
            "160/160 [==============================] - 0s 594us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 83/300\n",
            "160/160 [==============================] - 0s 620us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 84/300\n",
            "160/160 [==============================] - 0s 639us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 85/300\n",
            "160/160 [==============================] - 0s 706us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 86/300\n",
            "160/160 [==============================] - 0s 635us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 87/300\n",
            "160/160 [==============================] - 0s 576us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 88/300\n",
            "160/160 [==============================] - 0s 599us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 89/300\n",
            "160/160 [==============================] - 0s 631us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 90/300\n",
            "160/160 [==============================] - 0s 613us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 91/300\n",
            "160/160 [==============================] - 0s 597us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 92/300\n",
            "160/160 [==============================] - 0s 632us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 93/300\n",
            "160/160 [==============================] - 0s 581us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 94/300\n",
            "160/160 [==============================] - 0s 633us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 95/300\n",
            "160/160 [==============================] - 0s 646us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 96/300\n",
            "160/160 [==============================] - 0s 565us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 97/300\n",
            "160/160 [==============================] - 0s 603us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 98/300\n",
            "160/160 [==============================] - 0s 588us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 99/300\n",
            "160/160 [==============================] - 0s 566us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 100/300\n",
            "160/160 [==============================] - 0s 596us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 101/300\n",
            "160/160 [==============================] - 0s 606us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 102/300\n",
            "160/160 [==============================] - 0s 617us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 103/300\n",
            "160/160 [==============================] - 0s 591us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 104/300\n",
            "160/160 [==============================] - 0s 623us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 105/300\n",
            "160/160 [==============================] - 0s 673us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 106/300\n",
            "160/160 [==============================] - 0s 561us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 107/300\n",
            "160/160 [==============================] - 0s 604us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 108/300\n",
            "160/160 [==============================] - 0s 564us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 109/300\n",
            "160/160 [==============================] - 0s 565us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 110/300\n",
            "160/160 [==============================] - 0s 558us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 111/300\n",
            "160/160 [==============================] - 0s 569us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 112/300\n",
            "160/160 [==============================] - 0s 599us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 113/300\n",
            "160/160 [==============================] - 0s 582us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 114/300\n",
            "160/160 [==============================] - 0s 557us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 115/300\n",
            "160/160 [==============================] - 0s 575us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 116/300\n",
            "160/160 [==============================] - 0s 622us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 117/300\n",
            "160/160 [==============================] - 0s 579us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 118/300\n",
            "160/160 [==============================] - 0s 604us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 119/300\n",
            "160/160 [==============================] - 0s 564us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 120/300\n",
            "160/160 [==============================] - 0s 576us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 121/300\n",
            "160/160 [==============================] - 0s 557us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 122/300\n",
            "160/160 [==============================] - 0s 648us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 123/300\n",
            "160/160 [==============================] - 0s 704us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 124/300\n",
            "160/160 [==============================] - 0s 567us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 125/300\n",
            "160/160 [==============================] - 0s 583us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 126/300\n",
            "160/160 [==============================] - 0s 674us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 127/300\n",
            "160/160 [==============================] - 0s 630us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 128/300\n",
            "160/160 [==============================] - 0s 669us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 129/300\n",
            "160/160 [==============================] - 0s 693us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 130/300\n",
            "160/160 [==============================] - 0s 610us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 131/300\n",
            "160/160 [==============================] - 0s 574us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 132/300\n",
            "160/160 [==============================] - 0s 553us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 133/300\n",
            "160/160 [==============================] - 0s 561us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 134/300\n",
            "160/160 [==============================] - 0s 569us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 135/300\n",
            "160/160 [==============================] - 0s 574us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 136/300\n",
            "160/160 [==============================] - 0s 667us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 137/300\n",
            "160/160 [==============================] - 0s 619us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 138/300\n",
            "160/160 [==============================] - 0s 577us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 139/300\n",
            "160/160 [==============================] - 0s 580us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 140/300\n",
            "160/160 [==============================] - 0s 577us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 141/300\n",
            "160/160 [==============================] - 0s 615us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 142/300\n",
            "160/160 [==============================] - 0s 587us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 143/300\n",
            "160/160 [==============================] - 0s 551us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 144/300\n",
            "160/160 [==============================] - 0s 571us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 145/300\n",
            "160/160 [==============================] - 0s 544us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 146/300\n",
            "160/160 [==============================] - 0s 551us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 147/300\n",
            "160/160 [==============================] - 0s 621us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 148/300\n",
            "160/160 [==============================] - 0s 598us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 149/300\n",
            "160/160 [==============================] - 0s 583us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 150/300\n",
            "160/160 [==============================] - 0s 591us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 151/300\n",
            "160/160 [==============================] - 0s 599us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 152/300\n",
            "160/160 [==============================] - 0s 570us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 153/300\n",
            "160/160 [==============================] - 0s 571us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 154/300\n",
            "160/160 [==============================] - 0s 586us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 155/300\n",
            "160/160 [==============================] - 0s 561us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 156/300\n",
            "160/160 [==============================] - 0s 586us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 157/300\n",
            "160/160 [==============================] - 0s 620us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 158/300\n",
            "160/160 [==============================] - 0s 646us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 159/300\n",
            "160/160 [==============================] - 0s 592us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 160/300\n",
            "160/160 [==============================] - 0s 569us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 161/300\n",
            "160/160 [==============================] - 0s 583us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 162/300\n",
            "160/160 [==============================] - 0s 621us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 163/300\n",
            "160/160 [==============================] - 0s 607us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 164/300\n",
            "160/160 [==============================] - 0s 626us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 165/300\n",
            "160/160 [==============================] - 0s 619us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 166/300\n",
            "160/160 [==============================] - 0s 578us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 167/300\n",
            "160/160 [==============================] - 0s 593us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 168/300\n",
            "160/160 [==============================] - 0s 617us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 169/300\n",
            "160/160 [==============================] - 0s 587us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 170/300\n",
            "160/160 [==============================] - 0s 579us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 171/300\n",
            "160/160 [==============================] - 0s 594us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 172/300\n",
            "160/160 [==============================] - 0s 562us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 173/300\n",
            "160/160 [==============================] - 0s 595us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 174/300\n",
            "160/160 [==============================] - 0s 594us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 175/300\n",
            "160/160 [==============================] - 0s 591us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 176/300\n",
            "160/160 [==============================] - 0s 573us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 177/300\n",
            "160/160 [==============================] - 0s 619us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 178/300\n",
            "160/160 [==============================] - 0s 580us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 179/300\n",
            "160/160 [==============================] - 0s 615us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 180/300\n",
            "160/160 [==============================] - 0s 629us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 181/300\n",
            "160/160 [==============================] - 0s 592us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 182/300\n",
            "160/160 [==============================] - 0s 569us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 183/300\n",
            "160/160 [==============================] - 0s 608us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 184/300\n",
            "160/160 [==============================] - 0s 599us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 185/300\n",
            "160/160 [==============================] - 0s 607us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 186/300\n",
            "160/160 [==============================] - 0s 609us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 187/300\n",
            "160/160 [==============================] - 0s 574us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 188/300\n",
            "160/160 [==============================] - 0s 604us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 189/300\n",
            "160/160 [==============================] - 0s 650us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 190/300\n",
            "160/160 [==============================] - 0s 576us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 191/300\n",
            "160/160 [==============================] - 0s 631us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 192/300\n",
            "160/160 [==============================] - 0s 609us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 193/300\n",
            "160/160 [==============================] - 0s 674us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 194/300\n",
            "160/160 [==============================] - 0s 595us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 195/300\n",
            "160/160 [==============================] - 0s 558us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 196/300\n",
            "160/160 [==============================] - 0s 611us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 197/300\n",
            "160/160 [==============================] - 0s 579us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 198/300\n",
            "160/160 [==============================] - 0s 575us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 199/300\n",
            "160/160 [==============================] - 0s 669us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 200/300\n",
            "160/160 [==============================] - 0s 592us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 201/300\n",
            "160/160 [==============================] - 0s 571us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 202/300\n",
            "160/160 [==============================] - 0s 627us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 203/300\n",
            "160/160 [==============================] - 0s 595us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 204/300\n",
            "160/160 [==============================] - 0s 688us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 205/300\n",
            "160/160 [==============================] - 0s 579us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 206/300\n",
            "160/160 [==============================] - 0s 579us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 207/300\n",
            "160/160 [==============================] - 0s 619us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 208/300\n",
            "160/160 [==============================] - 0s 600us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 209/300\n",
            "160/160 [==============================] - 0s 592us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 210/300\n",
            "160/160 [==============================] - 0s 665us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 211/300\n",
            "160/160 [==============================] - 0s 595us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 212/300\n",
            "160/160 [==============================] - 0s 625us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 213/300\n",
            "160/160 [==============================] - 0s 642us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 214/300\n",
            "160/160 [==============================] - 0s 556us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 215/300\n",
            "160/160 [==============================] - 0s 578us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 216/300\n",
            "160/160 [==============================] - 0s 593us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 217/300\n",
            "160/160 [==============================] - 0s 584us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 218/300\n",
            "160/160 [==============================] - 0s 581us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 219/300\n",
            "160/160 [==============================] - 0s 574us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 220/300\n",
            "160/160 [==============================] - 0s 651us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 221/300\n",
            "160/160 [==============================] - 0s 584us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 222/300\n",
            "160/160 [==============================] - 0s 687us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 223/300\n",
            "160/160 [==============================] - 0s 633us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 224/300\n",
            "160/160 [==============================] - 0s 608us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 225/300\n",
            "160/160 [==============================] - 0s 589us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 226/300\n",
            "160/160 [==============================] - 0s 602us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 227/300\n",
            "160/160 [==============================] - 0s 617us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 228/300\n",
            "160/160 [==============================] - 0s 635us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 229/300\n",
            "160/160 [==============================] - 0s 682us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 230/300\n",
            "160/160 [==============================] - 0s 758us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 231/300\n",
            "160/160 [==============================] - 0s 597us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 232/300\n",
            "160/160 [==============================] - 0s 607us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 233/300\n",
            "160/160 [==============================] - 0s 668us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 234/300\n",
            "160/160 [==============================] - 0s 627us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 235/300\n",
            "160/160 [==============================] - 0s 595us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 236/300\n",
            "160/160 [==============================] - 0s 654us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 237/300\n",
            "160/160 [==============================] - 0s 640us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 238/300\n",
            "160/160 [==============================] - 0s 609us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 239/300\n",
            "160/160 [==============================] - 0s 640us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 240/300\n",
            "160/160 [==============================] - 0s 649us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 241/300\n",
            "160/160 [==============================] - 0s 605us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 242/300\n",
            "160/160 [==============================] - 0s 625us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 243/300\n",
            "160/160 [==============================] - 0s 618us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 244/300\n",
            "160/160 [==============================] - 0s 618us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 245/300\n",
            "160/160 [==============================] - 0s 593us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 246/300\n",
            "160/160 [==============================] - 0s 632us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 247/300\n",
            "160/160 [==============================] - 0s 596us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 248/300\n",
            "160/160 [==============================] - 0s 593us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 249/300\n",
            "160/160 [==============================] - 0s 630us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 250/300\n",
            "160/160 [==============================] - 0s 723us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 251/300\n",
            "160/160 [==============================] - 0s 637us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 252/300\n",
            "160/160 [==============================] - 0s 645us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 253/300\n",
            "160/160 [==============================] - 0s 630us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 254/300\n",
            "160/160 [==============================] - 0s 620us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 255/300\n",
            "160/160 [==============================] - 0s 685us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 256/300\n",
            "160/160 [==============================] - 0s 622us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 257/300\n",
            "160/160 [==============================] - 0s 625us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 258/300\n",
            "160/160 [==============================] - 0s 613us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 259/300\n",
            "160/160 [==============================] - 0s 622us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 260/300\n",
            "160/160 [==============================] - 0s 661us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 261/300\n",
            "160/160 [==============================] - 0s 627us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 262/300\n",
            "160/160 [==============================] - 0s 592us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 263/300\n",
            "160/160 [==============================] - 0s 601us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 264/300\n",
            "160/160 [==============================] - 0s 598us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 265/300\n",
            "160/160 [==============================] - 0s 598us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 266/300\n",
            "160/160 [==============================] - 0s 562us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 267/300\n",
            "160/160 [==============================] - 0s 624us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 268/300\n",
            "160/160 [==============================] - 0s 597us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 269/300\n",
            "160/160 [==============================] - 0s 573us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 270/300\n",
            "160/160 [==============================] - 0s 648us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 271/300\n",
            "160/160 [==============================] - 0s 627us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 272/300\n",
            "160/160 [==============================] - 0s 655us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 273/300\n",
            "160/160 [==============================] - 0s 681us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 274/300\n",
            "160/160 [==============================] - 0s 707us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 275/300\n",
            "160/160 [==============================] - 0s 701us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 276/300\n",
            "160/160 [==============================] - 0s 730us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 277/300\n",
            "160/160 [==============================] - 0s 661us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 278/300\n",
            "160/160 [==============================] - 0s 643us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 279/300\n",
            "160/160 [==============================] - 0s 732us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 280/300\n",
            "160/160 [==============================] - 0s 631us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 281/300\n",
            "160/160 [==============================] - 0s 624us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 282/300\n",
            "160/160 [==============================] - 0s 607us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 283/300\n",
            "160/160 [==============================] - 0s 621us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 284/300\n",
            "160/160 [==============================] - 0s 652us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 285/300\n",
            "160/160 [==============================] - 0s 639us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 286/300\n",
            "160/160 [==============================] - 0s 601us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 287/300\n",
            "160/160 [==============================] - 0s 646us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 288/300\n",
            "160/160 [==============================] - 0s 613us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 289/300\n",
            "160/160 [==============================] - 0s 658us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 290/300\n",
            "160/160 [==============================] - 0s 657us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 291/300\n",
            "160/160 [==============================] - 0s 634us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 292/300\n",
            "160/160 [==============================] - 0s 665us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 293/300\n",
            "160/160 [==============================] - 0s 677us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 294/300\n",
            "160/160 [==============================] - 0s 621us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 295/300\n",
            "160/160 [==============================] - 0s 646us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 296/300\n",
            "160/160 [==============================] - 0s 612us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 297/300\n",
            "160/160 [==============================] - 0s 590us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 298/300\n",
            "160/160 [==============================] - 0s 590us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 299/300\n",
            "160/160 [==============================] - 0s 731us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Epoch 300/300\n",
            "160/160 [==============================] - 0s 699us/step - loss: 8.0590 - acc: 0.5000 - val_loss: nan - val_acc: 0.5250\n",
            "Elapsed Time: 31.842384576797485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "d43023d26a87e3c4702b96bea5962c990c76aa0a",
        "id": "Wl7-odnFPIvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test, batch_size=80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "20cfe7f5891e3c26c599fa4cd728ac0a499ac70e",
        "id": "9E0V7SSAPIvR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "344f6636-4839-4ce1-cc8b-a6ef15b0720a"
      },
      "source": [
        "print(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1)))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.05      0.10        20\n",
            "           1       0.51      1.00      0.68        20\n",
            "\n",
            "    accuracy                           0.53        40\n",
            "   macro avg       0.76      0.53      0.39        40\n",
            "weighted avg       0.76      0.53      0.39        40\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "017f2431a45766fb0d4b2ef17ce613c1142ca085",
        "id": "dc0I9FBYPIvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(\"ranking-based average precision : {:.3f}\".format(label_ranking_average_precision_score(y_test.todense(), y_pred)))\n",
        "# print(\"Ranking loss : {:.3f}\".format(label_ranking_loss(y_test.todense(), y_pred)))\n",
        "# print(\"Coverage_error : {:.3f}\".format(coverage_error(y_test.todense(), y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "0fcf027f00a7031dd7dc7d25c0c6ff362c39954b",
        "id": "gnFeRvhZPIvZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "ef5450fe-afd4-42a7-deb6-952f0d8a6b35"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plt.figure(figsize=(5, 5))\n",
        "plot_confusion_matrix(cnf_matrix, classes=['0', '1'],\n",
        "                      title='Confusion matrix, without normalization')\n",
        "plt.show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAFgCAYAAACSQzOFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3dd7xcVbn/8c83gVBDDTUFkBIFlAAB\nLggKIjFEEC4/BSIgTQJc8F5FRBSkWS7XguXChRsEqQZURLoQReqPkkJCh9BJCIQQ6TXhuX/sdWAy\nnDJn5syZfVa+77z2KzO7rWfPzHlmzVp7r62IwMzMyqtfqwMwM7POOVGbmZWcE7WZWck5UZuZlZwT\ntZlZyTlRm5mVnBO1mVk3SRoq6R+SHpT0gKT/SPNXkjRR0oz0/4odbL9/WmeGpP27LM/nUZuZdY+k\nNYA1ImKqpIHAFGB34ABgXkScKulYYMWI+G7VtisBk4GRQKRtN4+If3ZUnmvUZmbdFBGzI2Jqevwa\n8BAwGNgNOD+tdj5F8q72BWBiRMxLyXkiMLqz8hbrqcDNzMqi/3JrRcx/q+7t460XHwDerpg1PiLG\nt7eupLWBTYG7gNUiYnZa9DywWjubDAaerXg+M83rkBO1mWUn5r/FEsP3rHv7t6ed8XZEjOxqPUnL\nApcB34yIVyV9GENESOqRtmU3fZhZhgTqV/9USwnS4hRJ+uKI+HOa/UJqv25rx57TzqazgKEVz4ek\neR1yojaz/AiQ6p+62n1RdT4HeCgiTqtYdCXQdhbH/sAV7Wx+PTBK0orprJBRaV6HnKjNzLrv08B+\nwOckTUvTGOBUYCdJM4DPp+dIGinptwARMQ/4ITApTaekeR3y6Xlmlp1+y6wWS3ziq3Vv//aUX02p\npY26t7gz0czyVEMTRl/hRG1mGVLNnYJ9QT5HYmaWKdeozSxPbvowMysxkVXThxO1mWWotvOh+won\najPLU0Y16nyOxMwsU65Rm1me3PRhZlZmeZ1H7URtZvlpG5QpE/l85ZiZZco1ajPLk5s+zMzKLK82\n6nyOpAdIWkrSVZJekfTHBvazj6QbejK2VpG0naRHylKepLUlhSRXMqpIekrS59Pj77eNf9zDZZwl\n6Qc9vd+m6Kf6p5Lpk4la0lclTZb0uqTZkq6TtG0P7PrLFDejXDkivlLvTiLi4ogY1QPxNFVKeOt1\ntk5E3BoRw3srpuryKpNPs0k6T9KPeqOsZouIn0TE1xvZh6QDJN1Wtd/DIuKHjUXXC9ouIW/irbh6\nU/ki6oKko4BfAT+hSKrDgP+huE17o9YCHo2I+T2wrz7Ptdbm8Wtr3dGnErWk5YFTgCMi4s8R8UZE\nvBcRV0XEd9I6S0j6laTn0vQrSUukZdtLminp25LmpNr4gWnZycAJwF6ppn6wpJMkXVRR/kI/u1ON\n4wlJr0l6UtI+FfNvq9huG0mTUpPKJEnbVCy7SdIPJd2e9nODpEEdHH9b/MdUxL+7pDGSHpU0T9L3\nK9bfUtIdkl5O654uaUBadktabXo63r0q9v9dSc8Dv2ubl7ZZN5WxWXq+pqQXJW1fw3t3vqRvp8eD\n0+t4RNV++1WVdyHFF/FVKcZjKna5j6RnJM2VdFxFOZ29/x+pIbb9qpA0DtgHOCaVdVUHxxGSDpM0\nI72uZ0jFeWAp/uMlPZ3enwvSZ7bys3OwpGeAGyvmHSjpWUn/TPveQtK9af+nV5S9rqQbJb2Ujvti\nSSt0EOcHn930vr9eMc2XdFJadqykx9Nn70FJ/5rmfwI4C9g6bfNymr/Qrw5Jh0h6LL1/V0pas5bX\nqlc08Z6Jva1PJWpga2BJ4PJO1jkO+BdgBLAJsCVwfMXy1YHlgcHAwcAZklaMiBMpaumXRsSyEXFO\nZ4FIWgb4DbBzRAwEtgGmtbPeSsA1ad2VgdOAayStXLHaV4EDgVWBAcDRnRS9OsVrMJjii+VsYF9g\nc2A74AeS1knrLgC+BQyieO12BP4NICI+k9bZJB3vpRX7X4ni18W4yoIj4nHgu8BFkpYGfgecHxE3\ndRJvm5uB7dPjzwJPAJ+peH5rRLxfVd5+wDPArinGn1Ys3hYYno7phJRYoOv3v10RMR64GPhpKmvX\nTlbfBdgC+BSwJ/CFNP+ANO0AfAxYFji9atvPAp+o2AZgK2B9YC+KX4vHUdxvbyNgT0mfTesJ+E9g\nzbSPocBJNRzbkemYlqV43f7JhzddfZzic7M8cDLFe7tGRDwEHAbckbb9yBeCpM+lePYE1gCeBi6p\nWq2j16rJmn8X8t5Uvog6tzIwt4umiX0obhY5JyJepPjw7Vex/L20/L2IuBZ4neIPvh7vAxtLWioi\nZkfEA+2s80VgRkRcGBHzI2IC8DBQmQh+FxGPRsRbwB8okkxH3gN+HBHvUfxRDAJ+HRGvpfIfpEhQ\nRMSUiLgzlfsU8L8UiaKrYzoxIt5J8SwkIs4GHgPuovjjPK56nQ7cDGwrqR9Fgv4pxQ1CSTHdXON+\n2pwcEW9FxHRgOumY6fr97wmnRsTLEfEM8A8+fL/2AU6LiCci4nXge8DeWriZ46T0S7Dytf1hRLwd\nETcAbwATUvyzgFuBTQEi4rGImJjemxcpvvS7ej8/IGkV4C/ANyLinrTPP0bEcxHxfvqynkHx5VaL\nfYBzI2JqRLyTjndrSWtXrNPRa9V8rlG3zEvAIHXevrcmxTd7m6fTvA/2UZXo36So+XRLRLxBUQM6\nDJgt6RpJH68hnraYBlc8f74b8bwUEQvS47Y/9hcqlr/Vtr2kDSRdLel5Sa9S/GJot1mlwosR8XYX\n65wNbAz8d/oD7VKqjb9B8Ye6HXA18Jyk4dSXqDt6zbp6/3tCd8pejKIvpc2z7eyv+v3r6P1cTdIl\nkmal9/Miun4/SdsuDvwJ+H1EXFIx/2sq7qD9cmre2LjWfVJ1vOnL6SXq/2xbB/paor4DeAfYvZN1\nnqP42d5mWJpXjzeApSuer165MCKuj4idKGqWD1MksK7iaYtpVp0xdceZFHGtHxHLAd+n+PncmU5v\nSy9pWYqf5+cAJ6WmnVrdTHFmzYBUW7wZ2B9YkXaajWqJpx2dvf8LvZ+SFno/6yirlrLns3DibaSM\nn6TtP5nez33p+v1s89/Aq1Q0A0lai+IzeyTFmU4rAPdX7LOrWBc63tQcuDK989numps+WiMiXqFo\nlz1DRSfa0pIWl7SzpLb2ywnA8ZJWUdEpdwJFzaMe04DPSBqWOoW+17Yg1W52Sx/OdyiaUN5vZx/X\nAhuoOKVwMUl7ARtS1CibbSDFH+frqbZ/eNXyFyjaUrvj18DkdOrXNRQdTsAHHVg3dbLtzRRJoa0j\n86b0/LaKXwnVuhtjZ+//dGAjSSMkLclH23freT2qy/6WpHXSF1pbn0dPnUU0kOJz9oqkwcB3atlI\n0qEUv1r2qeoHWIYiGb+Y1juQokbd5gVgiFIHdDsmAAem13MJiuO9KzWztVYjzR5u+mhcRPwCOIqi\nZvAixU/JIyna3gB+BEwG7gXuA6amefWUNRG4NO1rCgsn134pjueAeRR/CNWJkIh4iaJD5dsUPwuP\nAXaJiLn1xNRNR1N0VL5GUXO6tGr5ScD56Wfvnl3tTNJuwGg+PM6jgM2Uznah6Ny6vZNd3EyRbNoS\n9W0UNdxbOtyi6Kw6PsXYWSdrmw7f/4h4lOKsob9RtMXeVrXtOcCGqay/0H3nAhdSHM+TwNvAN+rY\nT0dOBjYDXqH4kvxzjduNpfgCeq7izI/vR8SDwC8ofqm+AHyShd+/G4EHgOclfeTzGhF/A34AXAbM\nBtYF9q7nwJoioxq1Ihr9tWdWkDQN2DF9OZm1TL/lh8YS2xxV9/Zv//WoKRExsgdDaohPurceExG9\n16NvtghxojazDOU1KJMTtZnlqYSdgvVyojaz/LQNypSJUiXqlQcNiqHDqk85tkXNvTNmtzoEa7F4\n+2XivTfzqRI3qFSJeuiwtbjx1rtaHYa12ODRp7Q6BGuxd6Y1OpS226jNzMrPbdRmZiXXxBq1pHMp\nLmSbExEbp3mX8uEAbysAL7d3yqqkpyguQlsAzK/lfG0najPLU3Nr1OdRDGF7QduMiNjrw6L1C4or\nSDuyQ3euTnaiNjPrpoi4pWo41w+kmyPsCXyup8rLp7XdzKyNGr5xwCAV92Vtm8Z1VWSF7YAXImJG\nB8sDuEHSlFr36xq1meWpsaaPuQ2M9TGWYmTBjmwbEbMkrQpMlPRwRHQ2MJkTtZnlqTdvz1hR5mLA\nHhS3xmtXGoudiJgj6XKKO+p0mqjd9GFm1nM+DzwcETPbWyhpGUkD2x4Doyhu1tApJ2ozy44oatT1\nTl3uX5pAMY73cEkzJR2cFu1NVbOHpDUlXZuergbcJmk6cDdwTUT8tavy3PRhZvkRtd+krA4RMbaD\n+Qe0M+85YEx6/AQf3oi5Zk7UZpah2mrGfYUTtZllKadE7TZqM7OSc43azLKUU43aidrMsuREbWZW\nZk0+66O3OVGbWXaU2Vkf7kw0Mys516jNLEs51aidqM0sS07UZmYll1Oidhu1mVnJuUZtZvnx6Xlm\nZuWXU9OHE7WZZSe386idqM0sSzklancmmpmVnGvUZpanfCrUTtRmliHl1fThRG1mWcopUbuN2sys\n5FyjNrMs5VSjdqI2s+z4PGozs74gnzztRG1mGcrsrA93JpqZlZxr1GaWpZxq1E7UZpYlJ2ozs7LL\nJ0+7jdrMrOxcozazLLnpw8ysxKS8Lnhx04eZZaktWdcz1bDvcyXNkXR/xbyTJM2SNC1NYzrYdrSk\nRyQ9JunYWo7FidrMstTMRA2cB4xuZ/4vI2JEmq5tJ6b+wBnAzsCGwFhJG3ZVmBO1mVk3RcQtwLw6\nNt0SeCwinoiId4FLgN262siJ2szypAam+h0p6d7UNLJiO8sHA89WPJ+Z5nXKidrMstRg08cgSZMr\npnE1FHkmsC4wApgN/KKnjsVnfZhZfhoflGluRIzszgYR8cIHxUtnA1e3s9osYGjF8yFpXqdcozYz\n6wGS1qh4+q/A/e2sNglYX9I6kgYAewNXdrVv16jNLDsCmnkataQJwPYUTSQzgROB7SWNAAJ4Cjg0\nrbsm8NuIGBMR8yUdCVwP9AfOjYgHuirPidrMMtTcC14iYmw7s8/pYN3ngDEVz68FPnLqXmecqM0s\nSxldmOhEbWZ58iXkZmbWa1yjNrP8yE0fZmalJqBfv3wytZs+SuAbh3+d4Wuvyae3GNHqUKyXnXXs\n7jx95TFMPv+ID+Z9ct3VuOnMQ5h03hH86dR9GLj0Ei2MsO+S6p/Kxom6BMbusz9/+Et7FzFZ7i68\n7h52O/rChead+d3dOf5/J7LFAWdw5S0P8q2xn25RdFYWTtQlsM2227Hiiiu1OgxrgdunP828V99a\naN56Q1fmtmlPAXDj5MfZffsuR8G0djR5mNNe5URtVjIPPTmHXbf7OAB77LAxQ1ZdvsUR9UENNHuU\nME83N1HXcycDs0Xdoaf+hXG7b8ntvz2MZZcawLvvLWh1SH1OcQl5PjXqpp31UXEng50oxlydJOnK\niHiwWWWa5eDRZ+ay67cvAIpmkJ233qDFEfVF5Uy49WpmjbquOxmYLepWWWEZoKgRHvu1z3L2FZNa\nHJG1WjPPo27vTgZbVa+UBuQeBzBk6LAmhlNehxywL7ffejMvvTSXjTdYm2OPO4F99z+o1WFZLzj/\nxC+z3abrMGj5pXnssm/zw3P/wbJLDeDQPbYE4IqbH+KCa+9pcZR9U0YV6tZf8BIR44HxACM22zxa\nHE5LnH3eRa0OwVpk/5P/1O78M/50Zy9Hkp+cmj6amajrupOBmVnDSnr2Rr2a2UZd150MzMxsYU2r\nUdd7JwMzs0a1nZ6Xi6a2UddzJwMzs56QUZ5ufWeimVkzuEZtZlZyGeVpj/VhZlZ2rlGbWX7kpg8z\ns1IrzvpodRQ9x4nazDLkQZnMzKwXuUZtZlnKqELtRG1mecqp6cOJ2szyk9mgTE7UZpad3Mb6cGei\nmVnJuUZtZlnKqUbtRG1mWcooTztRm1memlmjlnQusAswJyI2TvN+BuwKvAs8DhwYES+3s+1TwGvA\nAmB+RIzsqjy3UZtZftJZH/VONTgPGF01byKwcUR8CngU+F4n2+8QESNqSdLgRG1m1m0RcQswr2re\nDRExPz29k+I+sT3CidrMsqM01ke9Uw84CLiug2UB3CBpiqRxtezMbdRmlqUG8+0gSZMrno+PiPG1\nlavjgPnAxR2ssm1EzJK0KjBR0sOpht4hJ2ozy1K/xjL13FrbjytJOoCik3HHiIj21omIWen/OZIu\nB7YEOk3UbvowM+sBkkYDxwBfiog3O1hnGUkD2x4Do4D7u9q3E7WZZamZZ31ImgDcAQyXNFPSwcDp\nwECK5oxpks5K664p6dq06WrAbZKmA3cD10TEX7sqz00fZpYdNflWXBExtp3Z53Sw7nPAmPT4CWCT\n7pbnRG1mWernKxPNzMotp7E+3EZtZlZyrlGbWZYyqlA7UZtZfkRxdWIunKjNLEs5dSa6jdrMrORc\nozaz/PTc4Eql4ERtZlnKKE87UZtZfkTDgzKVihO1mWUpozztzkQzs7LrsEYtabnONoyIV3s+HDOz\nnrGodCY+QHHLmMqjbXsewLAmxmVmVrdu3KS2T+gwUUfE0N4MxMysJ+XUmVhTG7WkvSV9Pz0eImnz\n5oZlZmZtukzUkk4HdgD2S7PeBM5qZlBmZo1SA1PZ1HJ63jYRsZmkewAiYp6kAU2Oy8ysIYtKZ2Kb\n9yT1o+hARNLKwPtNjcrMrAHFBS+tjqLn1JKozwAuA1aRdDKwJ3ByU6MyM2vEojbWR0RcIGkK8Pk0\n6ysR0eXtzc3MrGfUegl5f+A9iuYPX81oZqWXUYW6prM+jgMmAGsCQ4DfS/peswMzM2uEUvNHPVPZ\n1FKj/hqwaUS8CSDpx8A9wH82MzAzs3rl1plYSzPGbBZO6IuleWZm1gs6G5TplxRt0vOAByRdn56P\nAib1TnhmZvUpYxNGvTpr+mg7s+MB4JqK+Xc2Lxwzs56RT5rufFCmc3ozEDOzniLlNShTl52JktYF\nfgxsCCzZNj8iNmhiXGZmDckoT9fUmXge8DuKXxI7A38ALm1iTGZmVqGWRL10RFwPEBGPR8TxFAnb\nzKy0FrXzqN9JgzI9LukwYBYwsLlhmZk1poT5tm611Ki/BSwD/DvwaeAQ4KBmBmVm1ggh+qn+qcv9\nS+dKmiPp/op5K0maKGlG+n/FDrbdP60zQ9L+tRxPl4k6Iu6KiNci4pmI2C8ivhQRt9eyczOzTJ0H\njK6adyzw94hYH/h7er4QSSsBJwJbAVsCJ3aU0Ct1dsHL5aQxqNsTEXt0tXMzs5Zo8s1tI+IWSWtX\nzd4N2D49Ph+4Cfhu1TpfACZGxDwASRMpEv6EzsrrrI369FoC7kn9JZZeotYB/Sxbr89rdQTWagvm\nN7yLFnQKrhYRbcNrPA+s1s46g4FnK57PTPM61dkFL3/vToRmZmXS4HjMgyRNrng+PiLG17pxRISk\nDlskusvVVzPLjmi4Rj03IkZ2c5sXJK0REbMlrQHMaWedWXzYPALF0NE3dbVj3wTAzKxnXAm0ncWx\nP3BFO+tcD4yStGLqRByV5nWq5kQtaYla1zUza7V+qn/qiqQJwB3AcEkzJR0MnArsJGkGxa0LT03r\njpT0W4DUifhDihFIJwGntHUsdqaWsT62BM4BlgeGSdoE+HpEfKPrwzEza41m3jggIsZ2sGjHdtad\nDHy94vm5wLndKa+WGvVvgF2Al1Ih04EdulOImVlvkvK6hLyWRN0vIp6umregGcGYmdlH1XLWx7Op\n+SMk9Qe+ATza3LDMzBqT0z0Ta0nUh1M0fwwDXgD+luaZmZVWCVsw6tZloo6IOcDevRCLmVmPKO5C\nnk+mruWsj7NpZ8yPiBjXlIjMzHpATheJ1NL08beKx0sC/8rC16qbmVkT1dL0sdBttyRdCNzWtIjM\nzHpARi0fdY31sQ7tjwplZlYKqvEGAH1FLW3U/+TDNup+wDzaGRDbzKxMMsrTnSdqFZfobEIx4hPA\n+xHRY0P3mZk1S07nUXfaMZqS8rURsSBNTtJmZr2sljbqaZI2jYh7mh6NmVkPWGTOo5a0WETMBzYF\nJkl6HHiD4jWIiNisl2I0M+u2jPJ0pzXqu4HNgC/1UixmZj2jxnGl+4rOErUAIuLxXorFzMza0Vmi\nXkXSUR0tjIjTmhCPmVmPEPlUqTtL1P2BZSGjozWzRULRmdjqKHpOZ4l6dkSc0muRmJn1oEUlUWd0\nmGa2qCnjLbXq1dkFLx+5SaOZmfW+DmvUtdzC3MysjBalNmozs75Ji84FL2ZmfVZOl5DndLcaM7Ms\nuUZtZtlxG7WZWR+QUcuHE7WZ5Uj0y+hSECdqM8uOyKtG7c5EM7OSc43azPKzCI1HbWbWZ/k8ajOz\nEmtro6536nL/0nBJ0yqmVyV9s2qd7SW9UrHOCfUej2vUZmbdFBGPACMAJPUHZgGXt7PqrRGxS6Pl\nOVGbWZZ6seljR+DxiHi6WQW46cPMstTMpo8qewMTOli2taTpkq6TtFG9x+IatZllRzRcCx0kaXLF\n8/ERMf4j5UgDgC8B32tnH1OBtSLidUljgL8A69cTjBO1meVHDd/hZW5EjKxhvZ2BqRHxQvWCiHi1\n4vG1kv5H0qCImNvdYNz0YWZWv7F00OwhaXWlbwtJW1Lk25fqKcQ1ajPLUrO7EiUtA+wEHFox7zCA\niDgL+DJwuKT5wFvA3hER9ZTlRG1m2SmGOW1uqo6IN4CVq+adVfH4dOD0nijLidrMspTPdYluozYz\nKz3XqM0sSxkN9eFEbWY5UqOn55WKE7WZZacHLngpFSdqM8tSTjXqnL50zMyy5Bq1mWUpn/q0E7WZ\n5ajxsT5KxYnazLKTW2diTsdiZpYlJ+qSuOH6v/KpjYaz0cfX42c/PbXV4VgvGLLaCvx1/L8z9bLj\nmPKn4zhi7PYArLjc0lx95pHcd8UJXH3mkawwcKnWBtpHSap7Khsn6hJYsGAB3/z3I7jiquu4594H\n+eMlE3jowQdbHZY12fwF73PsaX9ms//3Yz77tZ9z6F6f4eMfW52jD9yJm+5+hE/udgo33f0IRx84\nqtWh9klqYCobJ+oSmHT33ay77nqs87GPMWDAAL6y195cfdUVrQ7Lmuz5ua8y7eGZALz+5js8/OTz\nrLnKCuyy/ae46Kq7ALjoqrvYdYdPtTLMPqsXb8XVdE7UJfDcc7MYMmToB88HDx7CrFmzWhiR9bZh\na6zEiOFDmHT/U6y68kCen1vcHOT5ua+y6soDWxxd31N0JqruqWyalqglnStpjqT7m1WGWQ6WWWoA\nE37+db7z88t47Y23P7K8vqHmLSfNrFGfB4xu4v6zseaag5k589kPns+aNZPBgwe3MCLrLYst1o8J\nPz+ES6+bzBU3Tgdgzkuvsfqg5QBYfdByvDjvtVaG2Ge56aMGEXELMK9Z+8/JyC224LHHZvDUk0/y\n7rvv8sdLL+GLu3yp1WFZLzjrxH145Mnn+c1FN34w75qb72PfXbcCYN9dt+Lqm+5tVXh9mBr6VzYt\nv+BF0jhgHMDQYcNaHE1rLLbYYvzy16ez6xe/wIIFC9j/gIPYcKONWh2WNdk2Iz7GPrtsxX2PzuLO\nS44F4MTTr+Tnv5vIRf91EPvvvjXPzJ7Hvsec2+JI+6Yy1ozr1fJEHRHjgfEAm28+cpFtjRu98xhG\n7zym1WFYL/r/055gqU2PbHfZmMP+u5ejsTJreaI2M+tpbWd95MKJ2szyU9JOwXo18/S8CcAdwHBJ\nMyUd3KyyzMyq5XTWR9Nq1BExtln7NjPrShnP3qiXr0w0Mys5t1GbWXYE9MunQu1EbWZ5yqnpw4na\nzLJUxk7BermN2sys5FyjNrMsuenDzKzE3JloZlZ65RwFr15O1GaWn5JeYVgvdyaamdVB0lOS7pM0\nTdLkdpZL0m8kPSbpXkmb1VuWa9RmlqVeqlDvEBFzO1i2M7B+mrYCzkz/d5sTtZllp+hMbHnbx27A\nBRERwJ2SVpC0RkTM7u6O3PRhZllSAxMwSNLkimlcO0UEcIOkKR0sHww8W/F8ZprXba5Rm1meGqtQ\nz42IkV2ss21EzJK0KjBR0sPpXrE9zjVqM7M6RMSs9P8c4HJgy6pVZgFDK54PSfO6zYnazLLUzLuQ\nS1pG0sC2x8Ao4P6q1a4EvpbO/vgX4JV62qfBTR9mlqkm9yWuBlyuopDFgN9HxF8lHQYQEWcB1wJj\ngMeAN4ED6y3MidrMstTMPB0RTwCbtDP/rIrHARzRE+W56cPMrORcozazPLX8NOqe40RtZtkpzofO\nJ1M7UZtZfjIblMmJ2syylFGedmeimVnZuUZtZnnKqErtRG1mGfIdXszMSi+nzkS3UZuZlZxr1GaW\nnYpxpbPgRG1mecooUztRm1mW3JloZlZy7kw0M7Ne4xq1mWUpowq1E7WZZSiz0z6cqM0sSzl1JrqN\n2sys5FyjNrPsiLzO+nCiNrMsZZSnnajNLFMZZWonajPLkjsTzcys17hGbWZZcmeimVnJZZSnnajN\nLFMZZWq3UZuZlZxr1GaWnWKoj3yq1E7UZpYfuTPRzKz0MsrTTtRmlqmMMrU7E83MuknSUEn/kPSg\npAck/Uc762wv6RVJ09J0Qr3luUZtZhlSszsT5wPfjoipkgYCUyRNjIgHq9a7NSJ2abQwJ2ozy1Iz\nOxMjYjYwOz1+TdJDwGCgOlH3CDd9mFl21OAEDJI0uWIa12FZ0trApsBd7SzeWtJ0SddJ2qje43GN\n2szso+ZGxMiuVpK0LHAZ8M2IeLVq8VRgrYh4XdIY4C/A+vUE4xq1meWpwSp1l7uXFqdI0hdHxJ+r\nl0fEqxHxenp8LbC4pEH1HIpr1GaWpWZ2JkoScA7wUESc1sE6qwMvRERI2pKiYvxSPeU5UZtZlpp8\nZeKngf2A+yRNS/O+DwwDiIizgC8Dh0uaD7wF7B0RUU9hTtRmlqVm5umIuK2rIiLidOD0niivVIl6\n6tQpc5daXE+3Oo4WGgTMbXUQ1nL+HMBarQ6gTEqVqCNilVbH0EqSJtfS02x58+egB3hQJjOzviCf\nTO1EbWbZEXnVqH0edbmMb8SgJjkAAAYkSURBVHUAVgr+HNhCXKMukYjwH6j5c9BDMqpQO1GbWZ5y\navpwojazLPmeiWZmZZdPnnZnYqtJGi5pa0mLS+rf6nisdfz+W0dco24hSXsAPwFmpWmypPPaGS7R\nMiZpg4h4NCIWSOofEQtaHVMOMqpQu0bdKmmIxL2AgyNiR+AKYCjwXUnLtTQ46zWSdgGmSfo9QFuy\nbnFYfZ7U2FQ2TtSttRwfDiR+OXA1sDjw1TSMomVM0jLAkcA3gXclXQRO1j1FDfwrGyfqFomI94DT\ngD0kbRcR7wO3AdOAbVsanPWKiHgDOAj4PXA0sGRlsm5lbFlo8o0DepMTdWvdCtwA7CfpMxGxICJ+\nD6wJbNLa0Kw3RMRzEfF6RMwFDgWWakvWkjaT9PHWRmhl4M7EFoqItyVdDATwvfRH+Q6wGukOx7bo\niIiXJB0K/EzSw0B/YIcWh9VnlbBiXDcn6haLiH9KOpviNvOHAm8D+0bEC62NzFohIuZKuhfYGdgp\nIma2Oqa+KqdeHifqEoiId4F/SLqleBrvtzomaw1JKwJjgFERcV+r4+m7ytkpWC8n6hJxB5KlX1i7\nRsTbrY7FysOJ2qxknKQb5/GozcysV7lGbWZZyqlG7URtZlnKqTPRTR9mZiXnRL2Ik7RA0jRJ90v6\no6SlG9jX9pKuTo+/JOnYTtZdQdK/1VHGSZKOrnV+1TrnSfpyN8paW9L93Y3RSsCDMllm3oqIERGx\nMfAucFjlQhW6/TmJiCsj4tROVlkB6HaiNqtFI8N8lDBPO1HbQm4F1ks1yUckXQDcDwyVNErSHZKm\nppr3sgCSRkt6WNJUYI+2HUk6QNLp6fFqki6XND1N2wCnAuum2vzP0nrfkTRJ0r2STq7Y13GSHpV0\nGzC8q4OQdEjaz3RJl1X9Svi8pMlpf7uk9ftL+llF2Yc2+kJaCWSUqZ2oDQBJi1Fcttx2Ndz6wP9E\nxEbAG8DxwOcjYjNgMnCUpCWBs4Fdgc2B1TvY/W+AmyNiE2Az4AHgWODxVJv/jqRRqcwtgRHA5pI+\nI2lzYO80bwywRQ2H8+eI2CKV9xBwcMWytVMZXwTOSsdwMPBKRGyR9n+IpHVqKMesV/isD1tK0rT0\n+FbgHIrR+56OiDvT/H8BNgRuT8NkDwDuAD4OPBkRMwDSqG/j2injc8DX4IOrL19Jl0pXGpWme9Lz\nZSkS90Dg8oh4M5VxZQ3HtLGkH1E0rywLXF+x7A/pEv0Zkp5IxzAK+FRF+/XyqexHayjLSiqnsz6c\nqO2tiBhROSMl4zcqZwETI2Js1XoLbdcgAf8ZEf9bVcY369jXecDuETFd0gHA9hXLomrdSGV/IyIq\nEzqS1q6jbCuJMnYK1stNH1aLO4FPS1oPijuTSNoAeBhYW9K6ab2xHWz/d+DwtG1/ScsDr1HUlttc\nDxxU0fY9WNKqwC3A7pKWkjSQopmlKwOB2el2Z/tULfuKpH4p5o8Bj6SyD0/rI2mDdPcV68MyaqJ2\njdq6FhEvpprpBElLpNnHR8SjksYB10h6k6LpZGA7u/gPYLykg4EFwOERcYek29Ppb9eldupPAHek\nGv3rFMO9TpV0KTAdmANMqiHkHwB3AS+m/ytjega4m+I2aIelMcF/S9F2PVVF4S8Cu9f26lhplTHj\n1kkR1b8Ezcz6ts02Hxm33VnLd3r7lhnQb0pEjOxsHUmjgV9T3ODht9Wno6ZKzQUUHe0vAXtFxFP1\nxOOmDzPLUjNvbqvi5sNnUJwptSEwVtKGVasdDPwzItYDfgn8V73H4kRtZtlpG+a0iVcmbgk8FhFP\npBt/XALsVrXObsD56fGfgB1T01q3uY3azLIzdeqU65daXIMa2MWSkiZXPB8fEeMrng8Gnq14PhPY\nqmofH6wTEfMlvQKsDMztbjBO1GaWnYgY3eoYepKbPszMum8WMLTi+ZA0r9110pW/y1N0KnabE7WZ\nWfdNAtaXtI6kARTDHFRfNXslsH96/GXgxqjzNDs3fZiZdVNqcz6S4mKp/sC5EfGApFOAyRFxJcVw\nDBdKegyYR5HM6+LzqM3MSs5NH2ZmJedEbWZWck7UZmYl50RtZlZyTtRmZiXnRG1mVnJO1GZmJfd/\nNfeZQX1MdjoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSpLZNxLfZOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K.clear_session()\n",
        "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 2} ) \n",
        "sess = tf.Session(config=config) \n",
        "keras.backend.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlmYo1uT2noY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "51a48cb1-88db-4438-8ad3-1b8e7cfc1cb3"
      },
      "source": [
        "inp = Input(shape=(feature, depth))\n",
        "C = Conv1D(filters=32, kernel_size=5, strides=1)(inp)\n",
        "\n",
        "C11 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(C)\n",
        "A11 = Activation(\"relu\")(C11)\n",
        "C12 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A11)\n",
        "S11 = Add()([C12, C])\n",
        "A12 = Activation(\"relu\")(S11)\n",
        "M11 = MaxPooling1D(pool_size=5, strides=2)(A12)\n",
        "\n",
        "\n",
        "C21 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M11)\n",
        "A21 = Activation(\"relu\")(C21)\n",
        "C22 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A21)\n",
        "S21 = Add()([C22, M11])\n",
        "A22 = Activation(\"relu\")(S11)\n",
        "M21 = MaxPooling1D(pool_size=5, strides=2)(A22)\n",
        "\n",
        "\n",
        "C31 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M21)\n",
        "A31 = Activation(\"relu\")(C31)\n",
        "C32 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A31)\n",
        "S31 = Add()([C32, M21])\n",
        "A32 = Activation(\"relu\")(S31)\n",
        "M31 = MaxPooling1D(pool_size=5, strides=2)(A32)\n",
        "\n",
        "\n",
        "C41 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M31)\n",
        "A41 = Activation(\"relu\")(C41)\n",
        "C42 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A41)\n",
        "S41 = Add()([C42, M31])\n",
        "A42 = Activation(\"relu\")(S41)\n",
        "M41 = MaxPooling1D(pool_size=5, strides=2)(A42)\n",
        "\n",
        "\n",
        "C51 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M41)\n",
        "A51 = Activation(\"relu\")(C51)\n",
        "C52 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A51)\n",
        "S51 = Add()([C52, M41])\n",
        "A52 = Activation(\"relu\")(S51)\n",
        "M51 = MaxPooling1D(pool_size=5, strides=2)(A52)\n",
        "\n",
        "F1 = Flatten()(M51)\n",
        "\n",
        "D1 = Dense(32)(F1)\n",
        "A6 = Activation(\"relu\")(D1)\n",
        "D2 = Dense(32)(A6)\n",
        "D3 = Dense(2)(D2)\n",
        "A7 = Softmax()(D3)\n",
        "\n",
        "model = Model(inputs=inp, outputs=A7)\n",
        "model.summary()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 101, 1)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 97, 32)       192         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 97, 32)       5152        conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 97, 32)       0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 97, 32)       5152        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 97, 32)       0           conv1d_3[0][0]                   \n",
            "                                                                 conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 97, 32)       0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 47, 32)       0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 47, 32)       5152        max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 47, 32)       0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 47, 32)       5152        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 47, 32)       0           conv1d_7[0][0]                   \n",
            "                                                                 max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 47, 32)       0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 22, 32)       0           activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 22, 32)       5152        max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 22, 32)       0           conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_9 (Conv1D)               (None, 22, 32)       5152        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 22, 32)       0           conv1d_9[0][0]                   \n",
            "                                                                 max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 22, 32)       0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 9, 32)        0           activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 9, 32)        5152        max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 9, 32)        0           conv1d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_11 (Conv1D)              (None, 9, 32)        5152        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 9, 32)        0           conv1d_11[0][0]                  \n",
            "                                                                 max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 9, 32)        0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1D)  (None, 3, 32)        0           activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 96)           0           max_pooling1d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 32)           3104        flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32)           0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 32)           1056        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 2)            66          dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "softmax_1 (Softmax)             (None, 2)            0           dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 45,634\n",
            "Trainable params: 45,634\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxTfk0L-d43L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def exp_decay(epoch):\n",
        "    initial_lrate = 0.002\n",
        "    k = 0.75\n",
        "    t = n_obs//(batch_size)  # every epoch we do n_obs/batch_size iteration\n",
        "    lrate = initial_lrate * math.exp(-k*t)\n",
        "    return lrate\n",
        "\n",
        "lrate = LearningRateScheduler(exp_decay)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3FTx_K1eEAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adam = Adam(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fMcJwToeGLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=adam,  metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eFsbnlmclm8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "558785ba-d44c-4b0f-a958-142f21af3dd9"
      },
      "source": [
        "start_time = time.time()\n",
        "history = model.fit(X_train, y_train, \n",
        "                    epochs=300, \n",
        "                    batch_size=batch_size, \n",
        "                    verbose=1, \n",
        "                    validation_data=(X_test, y_test), \n",
        "                    callbacks=[lrate])\n",
        "\n",
        "elapsed_time_parallel = time.time() - start_time\n",
        "print(\"Elapsed Time:\",elapsed_time_parallel)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 160 samples, validate on 40 samples\n",
            "Epoch 1/300\n",
            "160/160 [==============================] - 4s 24ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 2/300\n",
            "160/160 [==============================] - 0s 134us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 3/300\n",
            "160/160 [==============================] - 0s 135us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 4/300\n",
            "160/160 [==============================] - 0s 130us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 5/300\n",
            "160/160 [==============================] - 0s 132us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 6/300\n",
            "160/160 [==============================] - 0s 137us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 7/300\n",
            "160/160 [==============================] - 0s 147us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 8/300\n",
            "160/160 [==============================] - 0s 145us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 9/300\n",
            "160/160 [==============================] - 0s 156us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 10/300\n",
            "160/160 [==============================] - 0s 134us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 11/300\n",
            "160/160 [==============================] - 0s 137us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 12/300\n",
            "160/160 [==============================] - 0s 142us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 13/300\n",
            "160/160 [==============================] - 0s 137us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 14/300\n",
            "160/160 [==============================] - 0s 128us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 15/300\n",
            "160/160 [==============================] - 0s 129us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 16/300\n",
            "160/160 [==============================] - 0s 162us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 17/300\n",
            "160/160 [==============================] - 0s 133us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 18/300\n",
            "160/160 [==============================] - 0s 141us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 19/300\n",
            "160/160 [==============================] - 0s 138us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 20/300\n",
            "160/160 [==============================] - 0s 147us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 21/300\n",
            "160/160 [==============================] - 0s 181us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 22/300\n",
            "160/160 [==============================] - 0s 159us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 23/300\n",
            "160/160 [==============================] - 0s 147us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 24/300\n",
            "160/160 [==============================] - 0s 145us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 25/300\n",
            "160/160 [==============================] - 0s 152us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 26/300\n",
            "160/160 [==============================] - 0s 144us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 27/300\n",
            "160/160 [==============================] - 0s 149us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 28/300\n",
            "160/160 [==============================] - 0s 131us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 29/300\n",
            "160/160 [==============================] - 0s 135us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 30/300\n",
            "160/160 [==============================] - 0s 132us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 31/300\n",
            "160/160 [==============================] - 0s 146us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 32/300\n",
            "160/160 [==============================] - 0s 151us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 33/300\n",
            "160/160 [==============================] - 0s 144us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 34/300\n",
            "160/160 [==============================] - 0s 148us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 35/300\n",
            "160/160 [==============================] - 0s 222us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 36/300\n",
            "160/160 [==============================] - 0s 144us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 37/300\n",
            "160/160 [==============================] - 0s 137us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 38/300\n",
            "160/160 [==============================] - 0s 139us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 39/300\n",
            "160/160 [==============================] - 0s 166us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 40/300\n",
            "160/160 [==============================] - 0s 168us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 41/300\n",
            "160/160 [==============================] - 0s 153us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 42/300\n",
            "160/160 [==============================] - 0s 171us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 43/300\n",
            "160/160 [==============================] - 0s 143us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 44/300\n",
            "160/160 [==============================] - 0s 136us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 45/300\n",
            "160/160 [==============================] - 0s 140us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 46/300\n",
            "160/160 [==============================] - 0s 133us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 47/300\n",
            "160/160 [==============================] - 0s 142us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 48/300\n",
            "160/160 [==============================] - 0s 144us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 49/300\n",
            "160/160 [==============================] - 0s 147us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 50/300\n",
            "160/160 [==============================] - 0s 142us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 51/300\n",
            "160/160 [==============================] - 0s 140us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 52/300\n",
            "160/160 [==============================] - 0s 144us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 53/300\n",
            "160/160 [==============================] - 0s 165us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 54/300\n",
            "160/160 [==============================] - 0s 159us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 55/300\n",
            "160/160 [==============================] - 0s 155us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 56/300\n",
            "160/160 [==============================] - 0s 153us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 57/300\n",
            "160/160 [==============================] - 0s 149us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 58/300\n",
            "160/160 [==============================] - 0s 144us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 59/300\n",
            "160/160 [==============================] - 0s 149us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 60/300\n",
            "160/160 [==============================] - 0s 159us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 61/300\n",
            "160/160 [==============================] - 0s 187us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 62/300\n",
            "160/160 [==============================] - 0s 168us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 63/300\n",
            "160/160 [==============================] - 0s 157us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 64/300\n",
            "160/160 [==============================] - 0s 163us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 65/300\n",
            "160/160 [==============================] - 0s 153us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 66/300\n",
            "160/160 [==============================] - 0s 157us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 67/300\n",
            "160/160 [==============================] - 0s 152us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 68/300\n",
            "160/160 [==============================] - 0s 145us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 69/300\n",
            "160/160 [==============================] - 0s 153us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 70/300\n",
            "160/160 [==============================] - 0s 160us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 71/300\n",
            "160/160 [==============================] - 0s 162us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 72/300\n",
            "160/160 [==============================] - 0s 151us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 73/300\n",
            "160/160 [==============================] - 0s 148us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 74/300\n",
            "160/160 [==============================] - 0s 196us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 75/300\n",
            "160/160 [==============================] - 0s 158us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 76/300\n",
            "160/160 [==============================] - 0s 158us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 77/300\n",
            "160/160 [==============================] - 0s 145us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 78/300\n",
            "160/160 [==============================] - 0s 139us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 79/300\n",
            "160/160 [==============================] - 0s 145us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 80/300\n",
            "160/160 [==============================] - 0s 148us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 81/300\n",
            "160/160 [==============================] - 0s 160us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 82/300\n",
            "160/160 [==============================] - 0s 150us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 83/300\n",
            "160/160 [==============================] - 0s 153us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 84/300\n",
            "160/160 [==============================] - 0s 148us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 85/300\n",
            "160/160 [==============================] - 0s 170us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 86/300\n",
            "160/160 [==============================] - 0s 137us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 87/300\n",
            "160/160 [==============================] - 0s 128us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 88/300\n",
            "160/160 [==============================] - 0s 140us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 89/300\n",
            "160/160 [==============================] - 0s 138us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 90/300\n",
            "160/160 [==============================] - 0s 160us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 91/300\n",
            "160/160 [==============================] - 0s 156us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 92/300\n",
            "160/160 [==============================] - 0s 146us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 93/300\n",
            "160/160 [==============================] - 0s 163us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 94/300\n",
            "160/160 [==============================] - 0s 149us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 95/300\n",
            "160/160 [==============================] - 0s 134us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 96/300\n",
            "160/160 [==============================] - 0s 136us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 97/300\n",
            "160/160 [==============================] - 0s 136us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 98/300\n",
            "160/160 [==============================] - 0s 159us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 99/300\n",
            "160/160 [==============================] - 0s 183us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 100/300\n",
            "160/160 [==============================] - 0s 167us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 101/300\n",
            "160/160 [==============================] - 0s 152us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 102/300\n",
            "160/160 [==============================] - 0s 143us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 103/300\n",
            "160/160 [==============================] - 0s 141us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 104/300\n",
            "160/160 [==============================] - 0s 160us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 105/300\n",
            "160/160 [==============================] - 0s 142us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 106/300\n",
            "160/160 [==============================] - 0s 149us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 107/300\n",
            "160/160 [==============================] - 0s 133us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 108/300\n",
            "160/160 [==============================] - 0s 149us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 109/300\n",
            "160/160 [==============================] - 0s 142us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 110/300\n",
            "160/160 [==============================] - 0s 146us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 111/300\n",
            "160/160 [==============================] - 0s 137us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 112/300\n",
            "160/160 [==============================] - 0s 143us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 113/300\n",
            "160/160 [==============================] - 0s 159us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 114/300\n",
            "160/160 [==============================] - 0s 195us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 115/300\n",
            "160/160 [==============================] - 0s 146us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 116/300\n",
            "160/160 [==============================] - 0s 176us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 117/300\n",
            "160/160 [==============================] - 0s 146us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 118/300\n",
            "160/160 [==============================] - 0s 152us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 119/300\n",
            "160/160 [==============================] - 0s 150us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 120/300\n",
            "160/160 [==============================] - 0s 153us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 121/300\n",
            "160/160 [==============================] - 0s 151us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 122/300\n",
            "160/160 [==============================] - 0s 150us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 123/300\n",
            "160/160 [==============================] - 0s 142us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 124/300\n",
            "160/160 [==============================] - 0s 146us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 125/300\n",
            "160/160 [==============================] - 0s 135us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 126/300\n",
            "160/160 [==============================] - 0s 139us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 127/300\n",
            "160/160 [==============================] - 0s 150us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 128/300\n",
            "160/160 [==============================] - 0s 176us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 129/300\n",
            "160/160 [==============================] - 0s 154us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 130/300\n",
            "160/160 [==============================] - 0s 153us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 131/300\n",
            "160/160 [==============================] - 0s 154us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 132/300\n",
            "160/160 [==============================] - 0s 131us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 133/300\n",
            "160/160 [==============================] - 0s 129us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 134/300\n",
            "160/160 [==============================] - 0s 128us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 135/300\n",
            "160/160 [==============================] - 0s 142us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 136/300\n",
            "160/160 [==============================] - 0s 142us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 137/300\n",
            "160/160 [==============================] - 0s 148us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 138/300\n",
            "160/160 [==============================] - 0s 142us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 139/300\n",
            "160/160 [==============================] - 0s 246us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 140/300\n",
            "160/160 [==============================] - 0s 136us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 141/300\n",
            "160/160 [==============================] - 0s 149us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 142/300\n",
            "160/160 [==============================] - 0s 148us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 143/300\n",
            "160/160 [==============================] - 0s 149us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 144/300\n",
            "160/160 [==============================] - 0s 157us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 145/300\n",
            "160/160 [==============================] - 0s 150us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 146/300\n",
            "160/160 [==============================] - 0s 163us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 147/300\n",
            "160/160 [==============================] - 0s 136us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 148/300\n",
            "160/160 [==============================] - 0s 128us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 149/300\n",
            "160/160 [==============================] - 0s 143us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 150/300\n",
            "160/160 [==============================] - 0s 143us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 151/300\n",
            "160/160 [==============================] - 0s 155us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 152/300\n",
            "160/160 [==============================] - 0s 151us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 153/300\n",
            "160/160 [==============================] - 0s 149us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 154/300\n",
            "160/160 [==============================] - 0s 153us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 155/300\n",
            "160/160 [==============================] - 0s 137us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 156/300\n",
            "160/160 [==============================] - 0s 130us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 157/300\n",
            "160/160 [==============================] - 0s 142us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 158/300\n",
            "160/160 [==============================] - 0s 141us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 159/300\n",
            "160/160 [==============================] - 0s 157us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 160/300\n",
            "160/160 [==============================] - 0s 144us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 161/300\n",
            "160/160 [==============================] - 0s 150us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 162/300\n",
            "160/160 [==============================] - 0s 150us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 163/300\n",
            "160/160 [==============================] - 0s 134us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 164/300\n",
            "160/160 [==============================] - 0s 141us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 165/300\n",
            "160/160 [==============================] - 0s 143us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 166/300\n",
            "160/160 [==============================] - 0s 147us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 167/300\n",
            "160/160 [==============================] - 0s 149us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 168/300\n",
            "160/160 [==============================] - 0s 156us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 169/300\n",
            "160/160 [==============================] - 0s 150us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 170/300\n",
            "160/160 [==============================] - 0s 167us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 171/300\n",
            "160/160 [==============================] - 0s 142us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 172/300\n",
            "160/160 [==============================] - 0s 138us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 173/300\n",
            "160/160 [==============================] - 0s 159us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 174/300\n",
            "160/160 [==============================] - 0s 147us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 175/300\n",
            "160/160 [==============================] - 0s 141us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 176/300\n",
            "160/160 [==============================] - 0s 152us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 177/300\n",
            "160/160 [==============================] - 0s 158us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 178/300\n",
            "160/160 [==============================] - 0s 171us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 179/300\n",
            "160/160 [==============================] - 0s 157us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 180/300\n",
            "160/160 [==============================] - 0s 148us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 181/300\n",
            "160/160 [==============================] - 0s 147us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 182/300\n",
            "160/160 [==============================] - 0s 136us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 183/300\n",
            "160/160 [==============================] - 0s 134us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 184/300\n",
            "160/160 [==============================] - 0s 152us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 185/300\n",
            "160/160 [==============================] - 0s 195us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 186/300\n",
            "160/160 [==============================] - 0s 147us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 187/300\n",
            "160/160 [==============================] - 0s 145us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 188/300\n",
            "160/160 [==============================] - 0s 160us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 189/300\n",
            "160/160 [==============================] - 0s 142us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 190/300\n",
            "160/160 [==============================] - 0s 125us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 191/300\n",
            "160/160 [==============================] - 0s 129us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 192/300\n",
            "160/160 [==============================] - 0s 152us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 193/300\n",
            "160/160 [==============================] - 0s 145us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 194/300\n",
            "160/160 [==============================] - 0s 139us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 195/300\n",
            "160/160 [==============================] - 0s 131us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 196/300\n",
            "160/160 [==============================] - 0s 136us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 197/300\n",
            "160/160 [==============================] - 0s 135us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 198/300\n",
            "160/160 [==============================] - 0s 142us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 199/300\n",
            "160/160 [==============================] - 0s 148us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 200/300\n",
            "160/160 [==============================] - 0s 148us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 201/300\n",
            "160/160 [==============================] - 0s 149us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 202/300\n",
            "160/160 [==============================] - 0s 146us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 203/300\n",
            "160/160 [==============================] - 0s 153us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 204/300\n",
            "160/160 [==============================] - 0s 157us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 205/300\n",
            "160/160 [==============================] - 0s 140us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 206/300\n",
            "160/160 [==============================] - 0s 144us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 207/300\n",
            "160/160 [==============================] - 0s 156us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 208/300\n",
            "160/160 [==============================] - 0s 147us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 209/300\n",
            "160/160 [==============================] - 0s 143us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 210/300\n",
            "160/160 [==============================] - 0s 149us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 211/300\n",
            "160/160 [==============================] - 0s 177us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 212/300\n",
            "160/160 [==============================] - 0s 138us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 213/300\n",
            "160/160 [==============================] - 0s 146us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 214/300\n",
            "160/160 [==============================] - 0s 148us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 215/300\n",
            "160/160 [==============================] - 0s 159us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 216/300\n",
            "160/160 [==============================] - 0s 144us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 217/300\n",
            "160/160 [==============================] - 0s 206us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 218/300\n",
            "160/160 [==============================] - 0s 155us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 219/300\n",
            "160/160 [==============================] - 0s 149us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 220/300\n",
            "160/160 [==============================] - 0s 152us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 221/300\n",
            "160/160 [==============================] - 0s 141us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 222/300\n",
            "160/160 [==============================] - 0s 146us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 223/300\n",
            "160/160 [==============================] - 0s 154us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 224/300\n",
            "160/160 [==============================] - 0s 156us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 225/300\n",
            "160/160 [==============================] - 0s 150us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 226/300\n",
            "160/160 [==============================] - 0s 147us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 227/300\n",
            "160/160 [==============================] - 0s 144us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 228/300\n",
            "160/160 [==============================] - 0s 174us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 229/300\n",
            "160/160 [==============================] - 0s 145us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 230/300\n",
            "160/160 [==============================] - 0s 138us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 231/300\n",
            "160/160 [==============================] - 0s 147us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 232/300\n",
            "160/160 [==============================] - 0s 141us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 233/300\n",
            "160/160 [==============================] - 0s 148us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 234/300\n",
            "160/160 [==============================] - 0s 162us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 235/300\n",
            "160/160 [==============================] - 0s 142us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 236/300\n",
            "160/160 [==============================] - 0s 147us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 237/300\n",
            "160/160 [==============================] - 0s 159us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 238/300\n",
            "160/160 [==============================] - 0s 135us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 239/300\n",
            "160/160 [==============================] - 0s 137us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 240/300\n",
            "160/160 [==============================] - 0s 136us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 241/300\n",
            "160/160 [==============================] - 0s 141us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 242/300\n",
            "160/160 [==============================] - 0s 135us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 243/300\n",
            "160/160 [==============================] - 0s 138us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 244/300\n",
            "160/160 [==============================] - 0s 162us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 245/300\n",
            "160/160 [==============================] - 0s 185us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 246/300\n",
            "160/160 [==============================] - 0s 146us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 247/300\n",
            "160/160 [==============================] - 0s 143us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 248/300\n",
            "160/160 [==============================] - 0s 145us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 249/300\n",
            "160/160 [==============================] - 0s 153us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 250/300\n",
            "160/160 [==============================] - 0s 163us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 251/300\n",
            "160/160 [==============================] - 0s 182us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 252/300\n",
            "160/160 [==============================] - 0s 148us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 253/300\n",
            "160/160 [==============================] - 0s 156us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 254/300\n",
            "160/160 [==============================] - 0s 160us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 255/300\n",
            "160/160 [==============================] - 0s 143us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 256/300\n",
            "160/160 [==============================] - 0s 196us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 257/300\n",
            "160/160 [==============================] - 0s 144us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 258/300\n",
            "160/160 [==============================] - 0s 140us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 259/300\n",
            "160/160 [==============================] - 0s 144us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 260/300\n",
            "160/160 [==============================] - 0s 141us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 261/300\n",
            "160/160 [==============================] - 0s 139us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 262/300\n",
            "160/160 [==============================] - 0s 146us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 263/300\n",
            "160/160 [==============================] - 0s 138us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 264/300\n",
            "160/160 [==============================] - 0s 135us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 265/300\n",
            "160/160 [==============================] - 0s 161us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 266/300\n",
            "160/160 [==============================] - 0s 145us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 267/300\n",
            "160/160 [==============================] - 0s 132us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 268/300\n",
            "160/160 [==============================] - 0s 139us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 269/300\n",
            "160/160 [==============================] - 0s 146us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 270/300\n",
            "160/160 [==============================] - 0s 139us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 271/300\n",
            "160/160 [==============================] - 0s 144us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 272/300\n",
            "160/160 [==============================] - 0s 131us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 273/300\n",
            "160/160 [==============================] - 0s 136us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 274/300\n",
            "160/160 [==============================] - 0s 155us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 275/300\n",
            "160/160 [==============================] - 0s 151us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 276/300\n",
            "160/160 [==============================] - 0s 147us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 277/300\n",
            "160/160 [==============================] - 0s 141us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 278/300\n",
            "160/160 [==============================] - 0s 149us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 279/300\n",
            "160/160 [==============================] - 0s 165us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 280/300\n",
            "160/160 [==============================] - 0s 138us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 281/300\n",
            "160/160 [==============================] - 0s 129us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 282/300\n",
            "160/160 [==============================] - 0s 158us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 283/300\n",
            "160/160 [==============================] - 0s 140us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 284/300\n",
            "160/160 [==============================] - 0s 140us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 285/300\n",
            "160/160 [==============================] - 0s 155us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 286/300\n",
            "160/160 [==============================] - 0s 138us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 287/300\n",
            "160/160 [==============================] - 0s 138us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 288/300\n",
            "160/160 [==============================] - 0s 144us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 289/300\n",
            "160/160 [==============================] - 0s 153us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 290/300\n",
            "160/160 [==============================] - 0s 142us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 291/300\n",
            "160/160 [==============================] - 0s 133us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 292/300\n",
            "160/160 [==============================] - 0s 189us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 293/300\n",
            "160/160 [==============================] - 0s 148us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 294/300\n",
            "160/160 [==============================] - 0s 155us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 295/300\n",
            "160/160 [==============================] - 0s 185us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 296/300\n",
            "160/160 [==============================] - 0s 184us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 297/300\n",
            "160/160 [==============================] - 0s 149us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 298/300\n",
            "160/160 [==============================] - 0s 155us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 299/300\n",
            "160/160 [==============================] - 0s 135us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Epoch 300/300\n",
            "160/160 [==============================] - 0s 146us/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0764 - val_acc: 0.5000\n",
            "Elapsed Time: 12.53186583518982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS-XJXjMdUOF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d752752-a87f-4886-811f-352ff0770fac"
      },
      "source": [
        "print(\"speedup: \", str(elapsed_time_serial/elapsed_time_parallel))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "speedup:  2.5409133001873676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NE7Htzuduxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}