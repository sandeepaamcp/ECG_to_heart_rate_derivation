{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Heart_rate_predictions_from_ArXiv_1805_00794_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeepaamcp/ECG_to_heart_rate_derivation/blob/dev/Heart_rate_predictions_from_ArXiv_1805_00794_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "fU1WKbabPItn",
        "colab_type": "code",
        "outputId": "9b9f219f-c861-4c09-f8df-85834f826a83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# THE MAIN SOURCE FOR THIS CODE IS https://www.kaggle.com/coni57/model-from-arxiv-1805-00794\n",
        "# THE ASSOCIATED RESEARCH PAPER : https://arxiv.org/pdf/1805.00794\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "import itertools\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, label_ranking_average_precision_score, label_ranking_loss, coverage_error \n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from scipy.signal import resample\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "import pickle\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Softmax, Add, Flatten, Activation# , Dropout\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "EJJfnMbxPItu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/datasets/test_results/csv/values.csv', header=None)\n",
        "# df2 = pd.read_csv(\"../input/mitbih_test.csv\", header=None)\n",
        "# df = pd.concat([df, df2], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "494fc8a26ba40beb73fc1a4f7b219b213fb7705e",
        "id": "GkNmHr8cPIty",
        "colab_type": "code",
        "outputId": "f632f255-aff5-4797-9282-6ffa9199ea02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>105.820</td>\n",
              "      <td>106.762</td>\n",
              "      <td>106.007</td>\n",
              "      <td>106.007</td>\n",
              "      <td>105.079</td>\n",
              "      <td>105.820</td>\n",
              "      <td>105.820</td>\n",
              "      <td>104.895</td>\n",
              "      <td>105.634</td>\n",
              "      <td>106.007</td>\n",
              "      <td>104.895</td>\n",
              "      <td>105.263</td>\n",
              "      <td>104.530</td>\n",
              "      <td>105.448</td>\n",
              "      <td>105.634</td>\n",
              "      <td>104.895</td>\n",
              "      <td>105.448</td>\n",
              "      <td>105.634</td>\n",
              "      <td>105.263</td>\n",
              "      <td>105.448</td>\n",
              "      <td>104.712</td>\n",
              "      <td>105.448</td>\n",
              "      <td>106.007</td>\n",
              "      <td>105.448</td>\n",
              "      <td>105.634</td>\n",
              "      <td>106.007</td>\n",
              "      <td>105.448</td>\n",
              "      <td>105.634</td>\n",
              "      <td>104.895</td>\n",
              "      <td>105.634</td>\n",
              "      <td>105.634</td>\n",
              "      <td>105.634</td>\n",
              "      <td>105.820</td>\n",
              "      <td>106.195</td>\n",
              "      <td>105.079</td>\n",
              "      <td>105.820</td>\n",
              "      <td>105.263</td>\n",
              "      <td>106.007</td>\n",
              "      <td>106.195</td>\n",
              "      <td>105.634</td>\n",
              "      <td>...</td>\n",
              "      <td>106.007</td>\n",
              "      <td>106.195</td>\n",
              "      <td>106.007</td>\n",
              "      <td>105.448</td>\n",
              "      <td>106.007</td>\n",
              "      <td>105.079</td>\n",
              "      <td>105.634</td>\n",
              "      <td>105.448</td>\n",
              "      <td>106.383</td>\n",
              "      <td>105.634</td>\n",
              "      <td>105.079</td>\n",
              "      <td>106.383</td>\n",
              "      <td>105.263</td>\n",
              "      <td>105.263</td>\n",
              "      <td>106.007</td>\n",
              "      <td>105.079</td>\n",
              "      <td>105.263</td>\n",
              "      <td>105.634</td>\n",
              "      <td>104.712</td>\n",
              "      <td>105.079</td>\n",
              "      <td>105.448</td>\n",
              "      <td>103.986</td>\n",
              "      <td>105.263</td>\n",
              "      <td>105.448</td>\n",
              "      <td>104.712</td>\n",
              "      <td>105.079</td>\n",
              "      <td>105.634</td>\n",
              "      <td>105.079</td>\n",
              "      <td>105.634</td>\n",
              "      <td>104.530</td>\n",
              "      <td>105.263</td>\n",
              "      <td>105.634</td>\n",
              "      <td>105.079</td>\n",
              "      <td>105.634</td>\n",
              "      <td>104.895</td>\n",
              "      <td>105.079</td>\n",
              "      <td>104.895</td>\n",
              "      <td>105.079</td>\n",
              "      <td>105.448</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>65.005</td>\n",
              "      <td>65.076</td>\n",
              "      <td>63.694</td>\n",
              "      <td>67.039</td>\n",
              "      <td>66.815</td>\n",
              "      <td>66.298</td>\n",
              "      <td>60.423</td>\n",
              "      <td>59.880</td>\n",
              "      <td>60.302</td>\n",
              "      <td>66.741</td>\n",
              "      <td>70.588</td>\n",
              "      <td>69.606</td>\n",
              "      <td>65.359</td>\n",
              "      <td>62.048</td>\n",
              "      <td>66.964</td>\n",
              "      <td>66.890</td>\n",
              "      <td>62.565</td>\n",
              "      <td>60.914</td>\n",
              "      <td>62.500</td>\n",
              "      <td>66.152</td>\n",
              "      <td>70.671</td>\n",
              "      <td>70.922</td>\n",
              "      <td>67.568</td>\n",
              "      <td>63.966</td>\n",
              "      <td>69.444</td>\n",
              "      <td>71.942</td>\n",
              "      <td>71.856</td>\n",
              "      <td>67.492</td>\n",
              "      <td>66.815</td>\n",
              "      <td>66.593</td>\n",
              "      <td>69.767</td>\n",
              "      <td>67.720</td>\n",
              "      <td>66.890</td>\n",
              "      <td>67.720</td>\n",
              "      <td>70.671</td>\n",
              "      <td>70.175</td>\n",
              "      <td>68.571</td>\n",
              "      <td>66.519</td>\n",
              "      <td>68.807</td>\n",
              "      <td>70.671</td>\n",
              "      <td>...</td>\n",
              "      <td>65.502</td>\n",
              "      <td>68.650</td>\n",
              "      <td>70.505</td>\n",
              "      <td>67.873</td>\n",
              "      <td>62.565</td>\n",
              "      <td>63.358</td>\n",
              "      <td>66.079</td>\n",
              "      <td>70.258</td>\n",
              "      <td>67.340</td>\n",
              "      <td>64.935</td>\n",
              "      <td>64.655</td>\n",
              "      <td>67.189</td>\n",
              "      <td>67.644</td>\n",
              "      <td>66.007</td>\n",
              "      <td>65.646</td>\n",
              "      <td>70.755</td>\n",
              "      <td>70.922</td>\n",
              "      <td>67.492</td>\n",
              "      <td>65.717</td>\n",
              "      <td>66.890</td>\n",
              "      <td>68.027</td>\n",
              "      <td>66.298</td>\n",
              "      <td>65.359</td>\n",
              "      <td>65.717</td>\n",
              "      <td>73.171</td>\n",
              "      <td>73.260</td>\n",
              "      <td>71.429</td>\n",
              "      <td>70.755</td>\n",
              "      <td>71.770</td>\n",
              "      <td>71.006</td>\n",
              "      <td>65.076</td>\n",
              "      <td>66.007</td>\n",
              "      <td>70.340</td>\n",
              "      <td>72.202</td>\n",
              "      <td>72.551</td>\n",
              "      <td>71.344</td>\n",
              "      <td>73.983</td>\n",
              "      <td>71.259</td>\n",
              "      <td>66.298</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>80.214</td>\n",
              "      <td>81.411</td>\n",
              "      <td>82.531</td>\n",
              "      <td>81.522</td>\n",
              "      <td>81.411</td>\n",
              "      <td>79.787</td>\n",
              "      <td>82.079</td>\n",
              "      <td>81.744</td>\n",
              "      <td>80.645</td>\n",
              "      <td>80.863</td>\n",
              "      <td>82.192</td>\n",
              "      <td>82.531</td>\n",
              "      <td>80.645</td>\n",
              "      <td>80.537</td>\n",
              "      <td>82.418</td>\n",
              "      <td>81.081</td>\n",
              "      <td>79.576</td>\n",
              "      <td>81.744</td>\n",
              "      <td>81.855</td>\n",
              "      <td>81.633</td>\n",
              "      <td>79.787</td>\n",
              "      <td>81.967</td>\n",
              "      <td>83.102</td>\n",
              "      <td>79.893</td>\n",
              "      <td>78.227</td>\n",
              "      <td>80.537</td>\n",
              "      <td>79.893</td>\n",
              "      <td>80.107</td>\n",
              "      <td>81.633</td>\n",
              "      <td>80.321</td>\n",
              "      <td>81.522</td>\n",
              "      <td>82.988</td>\n",
              "      <td>78.947</td>\n",
              "      <td>80.107</td>\n",
              "      <td>80.972</td>\n",
              "      <td>79.681</td>\n",
              "      <td>79.893</td>\n",
              "      <td>81.744</td>\n",
              "      <td>81.191</td>\n",
              "      <td>80.863</td>\n",
              "      <td>...</td>\n",
              "      <td>78.947</td>\n",
              "      <td>80.537</td>\n",
              "      <td>80.214</td>\n",
              "      <td>79.470</td>\n",
              "      <td>81.081</td>\n",
              "      <td>83.218</td>\n",
              "      <td>77.922</td>\n",
              "      <td>80.214</td>\n",
              "      <td>79.787</td>\n",
              "      <td>78.844</td>\n",
              "      <td>78.844</td>\n",
              "      <td>81.081</td>\n",
              "      <td>80.214</td>\n",
              "      <td>79.365</td>\n",
              "      <td>83.333</td>\n",
              "      <td>81.633</td>\n",
              "      <td>79.681</td>\n",
              "      <td>80.000</td>\n",
              "      <td>78.329</td>\n",
              "      <td>77.922</td>\n",
              "      <td>80.863</td>\n",
              "      <td>79.787</td>\n",
              "      <td>79.260</td>\n",
              "      <td>80.972</td>\n",
              "      <td>79.787</td>\n",
              "      <td>79.787</td>\n",
              "      <td>81.855</td>\n",
              "      <td>78.947</td>\n",
              "      <td>80.214</td>\n",
              "      <td>78.947</td>\n",
              "      <td>78.740</td>\n",
              "      <td>80.754</td>\n",
              "      <td>79.681</td>\n",
              "      <td>80.000</td>\n",
              "      <td>81.522</td>\n",
              "      <td>80.645</td>\n",
              "      <td>79.681</td>\n",
              "      <td>81.855</td>\n",
              "      <td>81.081</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>81.633</td>\n",
              "      <td>82.418</td>\n",
              "      <td>62.176</td>\n",
              "      <td>106.007</td>\n",
              "      <td>80.537</td>\n",
              "      <td>64.171</td>\n",
              "      <td>82.873</td>\n",
              "      <td>79.156</td>\n",
              "      <td>77.821</td>\n",
              "      <td>96.463</td>\n",
              "      <td>118.812</td>\n",
              "      <td>67.039</td>\n",
              "      <td>94.937</td>\n",
              "      <td>61.350</td>\n",
              "      <td>102.041</td>\n",
              "      <td>122.449</td>\n",
              "      <td>95.087</td>\n",
              "      <td>60.729</td>\n",
              "      <td>78.844</td>\n",
              "      <td>85.349</td>\n",
              "      <td>80.429</td>\n",
              "      <td>81.967</td>\n",
              "      <td>100.000</td>\n",
              "      <td>105.263</td>\n",
              "      <td>92.166</td>\n",
              "      <td>65.217</td>\n",
              "      <td>91.047</td>\n",
              "      <td>73.439</td>\n",
              "      <td>81.191</td>\n",
              "      <td>81.301</td>\n",
              "      <td>83.916</td>\n",
              "      <td>82.192</td>\n",
              "      <td>78.125</td>\n",
              "      <td>73.892</td>\n",
              "      <td>79.156</td>\n",
              "      <td>77.922</td>\n",
              "      <td>86.207</td>\n",
              "      <td>81.744</td>\n",
              "      <td>81.855</td>\n",
              "      <td>84.151</td>\n",
              "      <td>...</td>\n",
              "      <td>78.534</td>\n",
              "      <td>80.537</td>\n",
              "      <td>79.365</td>\n",
              "      <td>78.637</td>\n",
              "      <td>81.411</td>\n",
              "      <td>80.972</td>\n",
              "      <td>77.519</td>\n",
              "      <td>80.972</td>\n",
              "      <td>83.333</td>\n",
              "      <td>82.873</td>\n",
              "      <td>81.633</td>\n",
              "      <td>81.301</td>\n",
              "      <td>81.191</td>\n",
              "      <td>77.821</td>\n",
              "      <td>76.142</td>\n",
              "      <td>83.218</td>\n",
              "      <td>83.916</td>\n",
              "      <td>84.270</td>\n",
              "      <td>83.218</td>\n",
              "      <td>83.682</td>\n",
              "      <td>79.893</td>\n",
              "      <td>73.260</td>\n",
              "      <td>77.620</td>\n",
              "      <td>78.740</td>\n",
              "      <td>79.156</td>\n",
              "      <td>80.972</td>\n",
              "      <td>84.626</td>\n",
              "      <td>83.565</td>\n",
              "      <td>79.893</td>\n",
              "      <td>78.023</td>\n",
              "      <td>82.873</td>\n",
              "      <td>81.855</td>\n",
              "      <td>79.681</td>\n",
              "      <td>82.079</td>\n",
              "      <td>84.034</td>\n",
              "      <td>83.565</td>\n",
              "      <td>82.759</td>\n",
              "      <td>85.227</td>\n",
              "      <td>82.873</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>160.428</td>\n",
              "      <td>162.162</td>\n",
              "      <td>162.602</td>\n",
              "      <td>160.428</td>\n",
              "      <td>161.290</td>\n",
              "      <td>158.730</td>\n",
              "      <td>158.730</td>\n",
              "      <td>160.858</td>\n",
              "      <td>161.290</td>\n",
              "      <td>160.858</td>\n",
              "      <td>160.428</td>\n",
              "      <td>161.290</td>\n",
              "      <td>158.730</td>\n",
              "      <td>157.480</td>\n",
              "      <td>161.290</td>\n",
              "      <td>160.858</td>\n",
              "      <td>159.574</td>\n",
              "      <td>158.311</td>\n",
              "      <td>160.428</td>\n",
              "      <td>160.858</td>\n",
              "      <td>161.725</td>\n",
              "      <td>156.658</td>\n",
              "      <td>156.250</td>\n",
              "      <td>157.068</td>\n",
              "      <td>158.311</td>\n",
              "      <td>158.311</td>\n",
              "      <td>155.844</td>\n",
              "      <td>155.844</td>\n",
              "      <td>155.440</td>\n",
              "      <td>91.884</td>\n",
              "      <td>151.899</td>\n",
              "      <td>158.730</td>\n",
              "      <td>152.672</td>\n",
              "      <td>149.254</td>\n",
              "      <td>153.453</td>\n",
              "      <td>150.754</td>\n",
              "      <td>148.515</td>\n",
              "      <td>157.480</td>\n",
              "      <td>156.250</td>\n",
              "      <td>85.592</td>\n",
              "      <td>...</td>\n",
              "      <td>158.311</td>\n",
              "      <td>159.151</td>\n",
              "      <td>160.000</td>\n",
              "      <td>160.858</td>\n",
              "      <td>160.858</td>\n",
              "      <td>160.428</td>\n",
              "      <td>157.480</td>\n",
              "      <td>96.000</td>\n",
              "      <td>84.746</td>\n",
              "      <td>81.191</td>\n",
              "      <td>80.645</td>\n",
              "      <td>80.863</td>\n",
              "      <td>80.107</td>\n",
              "      <td>79.576</td>\n",
              "      <td>78.844</td>\n",
              "      <td>79.787</td>\n",
              "      <td>78.431</td>\n",
              "      <td>136.986</td>\n",
              "      <td>136.054</td>\n",
              "      <td>83.682</td>\n",
              "      <td>134.529</td>\n",
              "      <td>134.228</td>\n",
              "      <td>83.102</td>\n",
              "      <td>126.582</td>\n",
              "      <td>86.580</td>\n",
              "      <td>131.868</td>\n",
              "      <td>85.349</td>\n",
              "      <td>77.320</td>\n",
              "      <td>77.419</td>\n",
              "      <td>78.329</td>\n",
              "      <td>78.431</td>\n",
              "      <td>78.534</td>\n",
              "      <td>86.455</td>\n",
              "      <td>77.519</td>\n",
              "      <td>85.227</td>\n",
              "      <td>79.365</td>\n",
              "      <td>79.787</td>\n",
              "      <td>78.125</td>\n",
              "      <td>82.192</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 102 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0        1        2        3    ...      98       99       100  101\n",
              "0  105.820  106.762  106.007  106.007  ...  104.895  105.079  105.448    0\n",
              "1   65.005   65.076   63.694   67.039  ...   73.983   71.259   66.298    8\n",
              "2   80.214   81.411   82.531   81.522  ...   79.681   81.855   81.081    0\n",
              "3   81.633   82.418   62.176  106.007  ...   82.759   85.227   82.873    8\n",
              "4  160.428  162.162  162.602  160.428  ...   79.787   78.125   82.192    0\n",
              "\n",
              "[5 rows x 102 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "5281cb19f54f3bd379f875c24ae52b3b15fcafaf",
        "id": "a4DIT7vNPIt1",
        "colab_type": "code",
        "outputId": "26ef724f-2678-4f32-aca9-584248e1ed68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 350 entries, 0 to 349\n",
            "Columns: 102 entries, 0 to 101\n",
            "dtypes: float64(101), int64(1)\n",
            "memory usage: 279.0 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "0fac0fb658ea34b48055838b4ad85078e883360d",
        "id": "EB6HussvPIt5",
        "colab_type": "code",
        "outputId": "36c823cb-a3d0-4844-b30e-a0696921a3ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "df[101].value_counts()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    230\n",
              "8     83\n",
              "2     15\n",
              "1     14\n",
              "6      4\n",
              "3      3\n",
              "5      1\n",
              "Name: 101, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "147b7604bd8a389d7f6aa111f38ae308af7c4eb7",
        "id": "Z-5hcbF_PIt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "M = df.values\n",
        "X = M[:, :-1]\n",
        "y = M[:, -1].astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n82ksHULfRWA",
        "colab_type": "code",
        "outputId": "3dee9ab3-65b3-4ed9-a1e9-844fef0ef5c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "y"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 8, 0, 8, 0, 8, 0, 8, 0, 1, 0, 0, 8, 0, 8, 2, 8, 8, 8, 8, 1, 8,\n",
              "       8, 1, 8, 2, 8, 2, 2, 2, 0, 2, 0, 2, 0, 2, 5, 8, 2, 0, 0, 8, 8, 2,\n",
              "       2, 8, 8, 1, 8, 8, 0, 2, 0, 3, 1, 2, 8, 2, 8, 8, 8, 1, 8, 0, 8, 8,\n",
              "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 1, 8, 8, 8, 1, 8, 1, 8, 3,\n",
              "       8, 0, 8, 8, 8, 0, 8, 1, 6, 8, 8, 8, 8, 0, 6, 8, 0, 8, 6, 0, 6, 0,\n",
              "       0, 8, 0, 8, 8, 8, 8, 3, 8, 0, 8, 0, 8, 1, 8, 1, 8, 0, 8, 8, 0, 8,\n",
              "       0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0,\n",
              "       1, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 8, 0, 2, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "504d95532114efa4cc581d80bf02159c3ce519c6",
        "id": "HBPmVCNKPIuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# del df\n",
        "# del df2\n",
        "# del M"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "425c4b7abe39a14c6f81f8a71094cc1024276935",
        "id": "gAbFl72OPIuD",
        "colab_type": "text"
      },
      "source": [
        "# Visual Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "bcd502ecd1eb95bbf8af983396d3d0c3fb50ce4b",
        "id": "3daQE28mPIuF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d90f5267-c0c3-4c27-c8df-ac674df92fd9"
      },
      "source": [
        "C0 = np.argwhere(y == 0).flatten()\n",
        "C8 = np.argwhere(y == 8).flatten()\n",
        "\n",
        "print(C8)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  1   3   5   7  12  14  16  17  18  19  21  22  24  26  37  41  42  45\n",
            "  46  48  49  56  58  59  60  62  64  65  66  67  68  69  70  71  72  73\n",
            "  74  75  76  77  80  81  82  84  86  88  90  91  92  94  97  98  99 100\n",
            " 103 105 111 113 114 115 116 118 120 122 124 126 128 129 131 135 328 335\n",
            " 336 337 338 339 340 341 342 343 344 346 349]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rdjYeUkzUzw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "0c7b3f89-0ae1-4117-d94a-95afb3bd661a"
      },
      "source": [
        "print(X[C0,:])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[105.82  106.762 106.007 ... 104.895 105.079 105.448]\n",
            " [ 80.214  81.411  82.531 ...  79.681  81.855  81.081]\n",
            " [160.428 162.162 162.602 ...  79.787  78.125  82.192]\n",
            " ...\n",
            " [ 70.588  71.344  71.006 ...  71.77   70.838  70.838]\n",
            " [ 70.093  69.606  69.045 ...  65.862  63.694  62.959]\n",
            " [ 60.241  60.241  59.88  ...  63.559  63.694  61.1  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "85209065f110cea77f4d65053d1164e9ee816049",
        "id": "FqYklUdTPIuI",
        "colab_type": "code",
        "outputId": "76c4ecd6-ed1e-49fa-818c-aa1a74d86ce7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "x = np.arange(0, 101)\n",
        "X[C8, :][0].shape\n",
        "plt.figure(figsize=(20,2))\n",
        "plt.plot(x, X[C0, :][0], label=\"Cat. N\")\n",
        "# plt.plot(x, X[C1, :][0], label=\"Cat. S\")\n",
        "# plt.plot(x, X[C2, :][0], label=\"Cat. V\")\n",
        "# plt.plot(x, X[C3, :][0], label=\"Cat. F\")\n",
        "# plt.plot(x, X[C4, :][0], label=\"Cat. Q\")\n",
        "plt.legend()\n",
        "plt.title(\"1-beat Heart Rate for category Normal\", fontsize=20)\n",
        "plt.ylabel(\"Amplitude\", fontsize=15)\n",
        "plt.xlabel(\"Time (s)\", fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJcAAAC0CAYAAAApfs1jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1440x144 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-en-x08Aymk2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5e62afb5-07ea-4481-f35c-f4dab1a166f2"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[105.82  106.762 106.007 ... 104.895 105.079 105.448]\n",
            " [ 65.005  65.076  63.694 ...  73.983  71.259  66.298]\n",
            " [ 80.214  81.411  82.531 ...  79.681  81.855  81.081]\n",
            " ...\n",
            " [ 60.241  60.241  59.88  ...  63.559  63.694  61.1  ]\n",
            " [ 70.505  70.838  69.686 ...  68.182  66.593  65.717]\n",
            " [ 66.372  72.816  77.32  ...  61.538  62.762  70.755]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c42605d020fd51885437f4af3cf10cebbeafc9bb",
        "id": "foJsyFaTPIua",
        "colab_type": "text"
      },
      "source": [
        "# Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UJjD5as_YZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainMCI = np.random.choice(C0, 80)\n",
        "trainNorm = np.random.choice(C8, 80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkx6UgznBjiC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "f14b95cd-5683-4b27-f2f9-46b8fe94b9c6"
      },
      "source": [
        "trainNorm"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  7,   3,  14, 100,   7, 100, 120,  45,  84,  71, 337, 118,  26,\n",
              "        91,  37, 335, 341, 118,  77, 343, 346,  99,  59,  62, 115,  80,\n",
              "        66,  37,  86, 124, 328,  18,   1,  17, 120,  21, 344,  17,  72,\n",
              "        72,  70,  12,  80,  65,  16, 336, 335,  22,  71,  70,  91,  58,\n",
              "       118,  74,  84,  72, 124,  90, 341,   5,   1,  12,  26,  64,  18,\n",
              "       342,  37,  81, 340,  97, 120,  98,   7,  58,  37,  82,  66,  73,\n",
              "        24,  69])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "c136b567ed0cf450ed0476464c3b59d1ff1bb032",
        "id": "79ReBm0DPIub",
        "colab_type": "code",
        "outputId": "ee353c46-34ee-4aa9-c9f8-c0268ea390b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "testMCI = np.random.choice(C0, 20)\n",
        "testNorm = np.random.choice(C8, 20)\n",
        "testNorm"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([118,   1,  64, 118, 340,   5, 135, 335,  64,  18, 118,  74,  97,\n",
              "        84,  59, 342, 114,  69,  98, 118])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QvECgau0Lep",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b6c8307-937f-4f48-f4b0-fec05c028439"
      },
      "source": [
        "np.vstack([X[trainMCI], X[trainNorm]]).shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(160, 101)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SogjGTNmCOan",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "850101c0-df3a-4edf-d093-3b3539086779"
      },
      "source": [
        "np.hstack([y[trainMCI], y[trainNorm]]).shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(160,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K5zKxVG0dUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# np.hstack([y[subC0], y[subC1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "52573d0c3a715cd693e682227d01f5a73549e421",
        "id": "u_VqrWtBPIuh",
        "colab_type": "code",
        "outputId": "a15dc34a-6e2f-47a2-d1ab-8b8080150c81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "X_test = np.vstack([X[testMCI], X[testNorm]])\n",
        "y_test = np.hstack([y[testMCI], y[testNorm]])\n",
        "\n",
        "X_train = np.vstack([X[trainMCI], X[trainNorm]])\n",
        "y_train = np.hstack([y[trainMCI], y[trainNorm]])\n",
        "\n",
        "X_train, y_train = shuffle(X_train, y_train, random_state=0)\n",
        "X_test, y_test = shuffle(X_test, y_test, random_state=0)\n",
        "\n",
        "# del X\n",
        "# del y\n",
        "\n",
        "print(X_test.shape)\n",
        "print(X_train.shape)\n",
        "print(y_test.shape)\n",
        "print(y_train.shape)\n",
        "# print(X.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40, 101)\n",
            "(160, 101)\n",
            "(40,)\n",
            "(160,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKYLXEtUDGNK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f214f994-2791-4cf6-bebd-10d7f800e4eb"
      },
      "source": [
        "y_test"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([8, 8, 8, 0, 0, 0, 8, 0, 0, 8, 8, 8, 8, 0, 8, 8, 8, 0, 8, 0, 0, 0,\n",
              "       0, 0, 8, 0, 8, 0, 8, 0, 8, 8, 0, 8, 8, 0, 0, 8, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx0UcPr11Ib7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test = np.where(y_test > 1, 1, 0)\n",
        "y_train = np.where(y_train > 1, 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "7ae5108c9741b85f0f599cce51daf99df4733ed1",
        "id": "-yWqrtz6PIuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.expand_dims(X_train, 2)\n",
        "X_test = np.expand_dims(X_test, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "cbd350e57b2a44b4bf6a79f02c32dabb803e4855",
        "id": "nswRqohNPIuo",
        "colab_type": "code",
        "outputId": "1a2823c2-f0ab-4f0a-d80c-d027d15086d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"X_train\", X_train.shape)\n",
        "print(\"y_train\", y_train.shape)\n",
        "print(\"X_test\", X_test.shape)\n",
        "print(\"y_test\", y_test.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train (160, 101, 1)\n",
            "y_train (160,)\n",
            "X_test (40, 101, 1)\n",
            "y_test (40,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "5ddf1e7b397de3c413fc991945d2d7f09df67da1",
        "id": "CmHmd3RmPIur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ohe = OneHotEncoder()\n",
        "y_train = ohe.fit_transform(y_train.reshape(-1,1))\n",
        "y_test = ohe.transform(y_test.reshape(-1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "16c106c2702045790367fc49d7223560fc613d75",
        "id": "6jil13ZiPIuv",
        "colab_type": "code",
        "outputId": "85e9ef94-ad0e-4f1a-d701-cbaeea4f8ede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"X_train\", X_train.shape)\n",
        "print(\"y_train\", y_train.shape)\n",
        "print(\"X_test\", X_test.shape)\n",
        "print(\"y_test\", y_test.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train (160, 101, 1)\n",
            "y_train (160, 2)\n",
            "X_test (40, 101, 1)\n",
            "y_test (40, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c4de23b85abe34a726eab268171da0e827bafa35",
        "id": "erKYAY4bPIuy",
        "colab_type": "text"
      },
      "source": [
        "# Model\n",
        "\n",
        "Now let's re-create the model from the ArXiv Document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "fb0dc9775ddfa761c0ad948d59020fcbd2681c57",
        "id": "DrU2UcHvPIu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_obs, feature, depth = X_train.shape\n",
        "batch_size = 80"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "e70fab0b07290e042ba9cd7c6cba37462a457b03",
        "id": "W8xgao8gPIu6",
        "colab_type": "code",
        "outputId": "8ef29772-3b96-488b-9679-f475220218b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "inp = Input(shape=(feature, depth))\n",
        "C = Conv1D(filters=32, kernel_size=5, strides=1)(inp)\n",
        "\n",
        "C11 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(C)\n",
        "A11 = Activation(\"relu\")(C11)\n",
        "C12 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A11)\n",
        "S11 = Add()([C12, C])\n",
        "A12 = Activation(\"relu\")(S11)\n",
        "M11 = MaxPooling1D(pool_size=5, strides=2)(A12)\n",
        "\n",
        "\n",
        "C21 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M11)\n",
        "A21 = Activation(\"relu\")(C21)\n",
        "C22 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A21)\n",
        "S21 = Add()([C22, M11])\n",
        "A22 = Activation(\"relu\")(S11)\n",
        "M21 = MaxPooling1D(pool_size=5, strides=2)(A22)\n",
        "\n",
        "\n",
        "C31 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M21)\n",
        "A31 = Activation(\"relu\")(C31)\n",
        "C32 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A31)\n",
        "S31 = Add()([C32, M21])\n",
        "A32 = Activation(\"relu\")(S31)\n",
        "M31 = MaxPooling1D(pool_size=5, strides=2)(A32)\n",
        "\n",
        "\n",
        "C41 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M31)\n",
        "A41 = Activation(\"relu\")(C41)\n",
        "C42 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A41)\n",
        "S41 = Add()([C42, M31])\n",
        "A42 = Activation(\"relu\")(S41)\n",
        "M41 = MaxPooling1D(pool_size=5, strides=2)(A42)\n",
        "\n",
        "\n",
        "C51 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M41)\n",
        "A51 = Activation(\"relu\")(C51)\n",
        "C52 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A51)\n",
        "S51 = Add()([C52, M41])\n",
        "A52 = Activation(\"relu\")(S51)\n",
        "M51 = MaxPooling1D(pool_size=5, strides=2)(A52)\n",
        "\n",
        "F1 = Flatten()(M51)\n",
        "\n",
        "D1 = Dense(32)(F1)\n",
        "A6 = Activation(\"relu\")(D1)\n",
        "D2 = Dense(32)(A6)\n",
        "D3 = Dense(2)(D2)\n",
        "A7 = Softmax()(D3)\n",
        "\n",
        "model = Model(inputs=inp, outputs=A7)\n",
        "model.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 101, 1)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 97, 32)       192         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 97, 32)       5152        conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 97, 32)       0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 97, 32)       5152        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 97, 32)       0           conv1d_3[0][0]                   \n",
            "                                                                 conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 97, 32)       0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 47, 32)       0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 47, 32)       5152        max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 47, 32)       0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 47, 32)       5152        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 47, 32)       0           conv1d_7[0][0]                   \n",
            "                                                                 max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 47, 32)       0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 22, 32)       0           activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 22, 32)       5152        max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 22, 32)       0           conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_9 (Conv1D)               (None, 22, 32)       5152        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 22, 32)       0           conv1d_9[0][0]                   \n",
            "                                                                 max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 22, 32)       0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 9, 32)        0           activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 9, 32)        5152        max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 9, 32)        0           conv1d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_11 (Conv1D)              (None, 9, 32)        5152        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 9, 32)        0           conv1d_11[0][0]                  \n",
            "                                                                 max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 9, 32)        0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1D)  (None, 3, 32)        0           activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 96)           0           max_pooling1d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 32)           3104        flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32)           0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 32)           1056        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 2)            66          dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "softmax_1 (Softmax)             (None, 2)            0           dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 45,634\n",
            "Trainable params: 45,634\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "fdc0d8aa8475330af8d8e652d0b9ce214da66956",
        "id": "lazwYvDcPIu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def exp_decay(epoch):\n",
        "    initial_lrate = 0.002\n",
        "    k = 0.75\n",
        "    t = n_obs//(batch_size)  # every epoch we do n_obs/batch_size iteration\n",
        "    lrate = initial_lrate * math.exp(-k*t)\n",
        "    return lrate\n",
        "\n",
        "lrate = LearningRateScheduler(exp_decay)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "abcd2f0e8488c8f3b33cd6ed9ca7fd60fa44404b",
        "id": "Jom7PrMmPIvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adam = Adam(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "812e637ae56f6a4be9c8e98d3b501cfb11ef78cb",
        "id": "Kwc24PDmPIvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=adam,  metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "1e6ef643f6a55ab5b3c1b46126832c3ac45a6a2b",
        "id": "www7l_uDPIvL",
        "colab_type": "code",
        "outputId": "92e3a89a-5311-478d-cb69-de84ba785825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# It is observed that sometimes the accuracy won't change for epochs. \n",
        "# This is likely to happen due to Adam optimizer found a local minima\n",
        "# Source : https://stackoverflow.com/questions/37213388/keras-accuracy-does-not-change/53397560#53397560\n",
        "history = model.fit(X_train, y_train, \n",
        "                    epochs=300, \n",
        "                    batch_size=batch_size, \n",
        "                    verbose=1, \n",
        "                    validation_data=(X_test, y_test), \n",
        "                    callbacks=[lrate])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Train on 160 samples, validate on 40 samples\n",
            "Epoch 1/300\n",
            "160/160 [==============================] - 1s 4ms/step - loss: 16.3956 - accuracy: 0.5000 - val_loss: nan - val_accuracy: 0.5250\n",
            "Epoch 2/300\n",
            "160/160 [==============================] - 0s 745us/step - loss: 8.0236 - accuracy: 0.5063 - val_loss: nan - val_accuracy: 0.5250\n",
            "Epoch 3/300\n",
            "160/160 [==============================] - 0s 661us/step - loss: 3.2250 - accuracy: 0.5688 - val_loss: nan - val_accuracy: 0.5000\n",
            "Epoch 4/300\n",
            "160/160 [==============================] - 0s 661us/step - loss: 3.6954 - accuracy: 0.4875 - val_loss: nan - val_accuracy: 0.6000\n",
            "Epoch 5/300\n",
            "160/160 [==============================] - 0s 685us/step - loss: 1.8845 - accuracy: 0.5875 - val_loss: nan - val_accuracy: 0.5250\n",
            "Epoch 6/300\n",
            "160/160 [==============================] - 0s 695us/step - loss: 2.6400 - accuracy: 0.5125 - val_loss: nan - val_accuracy: 0.5750\n",
            "Epoch 7/300\n",
            "160/160 [==============================] - 0s 686us/step - loss: 1.6574 - accuracy: 0.5437 - val_loss: nan - val_accuracy: 0.4750\n",
            "Epoch 8/300\n",
            "160/160 [==============================] - 0s 721us/step - loss: 1.5365 - accuracy: 0.5562 - val_loss: nan - val_accuracy: 0.6000\n",
            "Epoch 9/300\n",
            "160/160 [==============================] - 0s 695us/step - loss: 1.2926 - accuracy: 0.5375 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 10/300\n",
            "160/160 [==============================] - 0s 701us/step - loss: 0.9562 - accuracy: 0.5437 - val_loss: nan - val_accuracy: 0.4750\n",
            "Epoch 11/300\n",
            "160/160 [==============================] - 0s 757us/step - loss: 1.1095 - accuracy: 0.6250 - val_loss: nan - val_accuracy: 0.6000\n",
            "Epoch 12/300\n",
            "160/160 [==============================] - 0s 704us/step - loss: 0.8376 - accuracy: 0.5625 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 13/300\n",
            "160/160 [==============================] - 0s 690us/step - loss: 0.8257 - accuracy: 0.6313 - val_loss: nan - val_accuracy: 0.6500\n",
            "Epoch 14/300\n",
            "160/160 [==============================] - 0s 704us/step - loss: 0.8072 - accuracy: 0.6812 - val_loss: nan - val_accuracy: 0.5500\n",
            "Epoch 15/300\n",
            "160/160 [==============================] - 0s 680us/step - loss: 0.6787 - accuracy: 0.5813 - val_loss: nan - val_accuracy: 0.6000\n",
            "Epoch 16/300\n",
            "160/160 [==============================] - 0s 664us/step - loss: 0.6813 - accuracy: 0.6875 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 17/300\n",
            "160/160 [==============================] - 0s 711us/step - loss: 0.6114 - accuracy: 0.7250 - val_loss: nan - val_accuracy: 0.6000\n",
            "Epoch 18/300\n",
            "160/160 [==============================] - 0s 666us/step - loss: 0.6478 - accuracy: 0.6000 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 19/300\n",
            "160/160 [==============================] - 0s 670us/step - loss: 0.6182 - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 20/300\n",
            "160/160 [==============================] - 0s 664us/step - loss: 0.5699 - accuracy: 0.7125 - val_loss: nan - val_accuracy: 0.6500\n",
            "Epoch 21/300\n",
            "160/160 [==============================] - 0s 704us/step - loss: 0.6462 - accuracy: 0.6187 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 22/300\n",
            "160/160 [==============================] - 0s 668us/step - loss: 0.6035 - accuracy: 0.7250 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 23/300\n",
            "160/160 [==============================] - 0s 669us/step - loss: 0.5846 - accuracy: 0.6687 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 24/300\n",
            "160/160 [==============================] - 0s 714us/step - loss: 0.5958 - accuracy: 0.6187 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 25/300\n",
            "160/160 [==============================] - 0s 684us/step - loss: 0.6011 - accuracy: 0.7125 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 26/300\n",
            "160/160 [==============================] - 0s 663us/step - loss: 0.5975 - accuracy: 0.6562 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 27/300\n",
            "160/160 [==============================] - 0s 673us/step - loss: 0.6243 - accuracy: 0.6875 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 28/300\n",
            "160/160 [==============================] - 0s 680us/step - loss: 0.5429 - accuracy: 0.7437 - val_loss: nan - val_accuracy: 0.6500\n",
            "Epoch 29/300\n",
            "160/160 [==============================] - 0s 670us/step - loss: 0.5684 - accuracy: 0.6562 - val_loss: nan - val_accuracy: 0.6000\n",
            "Epoch 30/300\n",
            "160/160 [==============================] - 0s 703us/step - loss: 0.5440 - accuracy: 0.7375 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 31/300\n",
            "160/160 [==============================] - 0s 653us/step - loss: 0.5679 - accuracy: 0.7000 - val_loss: nan - val_accuracy: 0.6500\n",
            "Epoch 32/300\n",
            "160/160 [==============================] - 0s 679us/step - loss: 0.5132 - accuracy: 0.7063 - val_loss: nan - val_accuracy: 0.6500\n",
            "Epoch 33/300\n",
            "160/160 [==============================] - 0s 660us/step - loss: 0.5319 - accuracy: 0.7125 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 34/300\n",
            "160/160 [==============================] - 0s 707us/step - loss: 0.5186 - accuracy: 0.7000 - val_loss: nan - val_accuracy: 0.6000\n",
            "Epoch 35/300\n",
            "160/160 [==============================] - 0s 688us/step - loss: 0.5102 - accuracy: 0.7312 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 36/300\n",
            "160/160 [==============================] - 0s 675us/step - loss: 0.4834 - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 37/300\n",
            "160/160 [==============================] - 0s 689us/step - loss: 0.5037 - accuracy: 0.7063 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 38/300\n",
            "160/160 [==============================] - 0s 659us/step - loss: 0.4897 - accuracy: 0.7375 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 39/300\n",
            "160/160 [==============================] - 0s 678us/step - loss: 0.4830 - accuracy: 0.7125 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 40/300\n",
            "160/160 [==============================] - 0s 698us/step - loss: 0.4994 - accuracy: 0.7563 - val_loss: nan - val_accuracy: 0.6500\n",
            "Epoch 41/300\n",
            "160/160 [==============================] - 0s 684us/step - loss: 0.4758 - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.6500\n",
            "Epoch 42/300\n",
            "160/160 [==============================] - 0s 669us/step - loss: 0.4624 - accuracy: 0.7437 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 43/300\n",
            "160/160 [==============================] - 0s 680us/step - loss: 0.4665 - accuracy: 0.7750 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 44/300\n",
            "160/160 [==============================] - 0s 688us/step - loss: 0.4759 - accuracy: 0.7000 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 45/300\n",
            "160/160 [==============================] - 0s 684us/step - loss: 0.4910 - accuracy: 0.7750 - val_loss: nan - val_accuracy: 0.6500\n",
            "Epoch 46/300\n",
            "160/160 [==============================] - 0s 678us/step - loss: 0.5020 - accuracy: 0.6750 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 47/300\n",
            "160/160 [==============================] - 0s 747us/step - loss: 0.4467 - accuracy: 0.7750 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 48/300\n",
            "160/160 [==============================] - 0s 657us/step - loss: 0.4917 - accuracy: 0.7063 - val_loss: nan - val_accuracy: 0.6500\n",
            "Epoch 49/300\n",
            "160/160 [==============================] - 0s 657us/step - loss: 0.4558 - accuracy: 0.7437 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 50/300\n",
            "160/160 [==============================] - 0s 680us/step - loss: 0.4920 - accuracy: 0.7250 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 51/300\n",
            "160/160 [==============================] - 0s 671us/step - loss: 0.4313 - accuracy: 0.7812 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 52/300\n",
            "160/160 [==============================] - 0s 667us/step - loss: 0.4467 - accuracy: 0.7688 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 53/300\n",
            "160/160 [==============================] - 0s 719us/step - loss: 0.4606 - accuracy: 0.7000 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 54/300\n",
            "160/160 [==============================] - 0s 681us/step - loss: 0.5127 - accuracy: 0.7750 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 55/300\n",
            "160/160 [==============================] - 0s 684us/step - loss: 0.4829 - accuracy: 0.7063 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 56/300\n",
            "160/160 [==============================] - 0s 704us/step - loss: 0.5172 - accuracy: 0.7500 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 57/300\n",
            "160/160 [==============================] - 0s 674us/step - loss: 0.4942 - accuracy: 0.7250 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 58/300\n",
            "160/160 [==============================] - 0s 676us/step - loss: 0.4618 - accuracy: 0.7875 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 59/300\n",
            "160/160 [==============================] - 0s 675us/step - loss: 0.5051 - accuracy: 0.7312 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 60/300\n",
            "160/160 [==============================] - 0s 680us/step - loss: 0.5050 - accuracy: 0.7125 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 61/300\n",
            "160/160 [==============================] - 0s 673us/step - loss: 0.5342 - accuracy: 0.7312 - val_loss: nan - val_accuracy: 0.6500\n",
            "Epoch 62/300\n",
            "160/160 [==============================] - 0s 700us/step - loss: 0.4307 - accuracy: 0.7750 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 63/300\n",
            "160/160 [==============================] - 0s 681us/step - loss: 0.4458 - accuracy: 0.7875 - val_loss: nan - val_accuracy: 0.6500\n",
            "Epoch 64/300\n",
            "160/160 [==============================] - 0s 674us/step - loss: 0.4669 - accuracy: 0.7250 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 65/300\n",
            "160/160 [==============================] - 0s 694us/step - loss: 0.4805 - accuracy: 0.8062 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 66/300\n",
            "160/160 [==============================] - 0s 688us/step - loss: 0.5258 - accuracy: 0.6687 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 67/300\n",
            "160/160 [==============================] - 0s 834us/step - loss: 0.5299 - accuracy: 0.7875 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 68/300\n",
            "160/160 [==============================] - 0s 725us/step - loss: 0.5450 - accuracy: 0.6875 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 69/300\n",
            "160/160 [==============================] - 0s 684us/step - loss: 0.4477 - accuracy: 0.7688 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 70/300\n",
            "160/160 [==============================] - 0s 670us/step - loss: 0.5000 - accuracy: 0.7563 - val_loss: nan - val_accuracy: 0.6250\n",
            "Epoch 71/300\n",
            "160/160 [==============================] - 0s 684us/step - loss: 0.5257 - accuracy: 0.7437 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 72/300\n",
            "160/160 [==============================] - 0s 691us/step - loss: 0.4897 - accuracy: 0.7500 - val_loss: nan - val_accuracy: 0.6500\n",
            "Epoch 73/300\n",
            "160/160 [==============================] - 0s 661us/step - loss: 0.4488 - accuracy: 0.7750 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 74/300\n",
            "160/160 [==============================] - 0s 666us/step - loss: 0.4122 - accuracy: 0.7875 - val_loss: nan - val_accuracy: 0.6000\n",
            "Epoch 75/300\n",
            "160/160 [==============================] - 0s 689us/step - loss: 0.4425 - accuracy: 0.7625 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 76/300\n",
            "160/160 [==============================] - 0s 665us/step - loss: 0.4544 - accuracy: 0.7937 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 77/300\n",
            "160/160 [==============================] - 0s 658us/step - loss: 0.4124 - accuracy: 0.7625 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 78/300\n",
            "160/160 [==============================] - 0s 684us/step - loss: 0.3993 - accuracy: 0.7937 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 79/300\n",
            "160/160 [==============================] - 0s 673us/step - loss: 0.3967 - accuracy: 0.7937 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 80/300\n",
            "160/160 [==============================] - 0s 664us/step - loss: 0.4072 - accuracy: 0.7875 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 81/300\n",
            "160/160 [==============================] - 0s 694us/step - loss: 0.3961 - accuracy: 0.7937 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 82/300\n",
            "160/160 [==============================] - 0s 693us/step - loss: 0.3971 - accuracy: 0.7875 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 83/300\n",
            "160/160 [==============================] - 0s 769us/step - loss: 0.4111 - accuracy: 0.7937 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 84/300\n",
            "160/160 [==============================] - 0s 735us/step - loss: 0.4578 - accuracy: 0.7563 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 85/300\n",
            "160/160 [==============================] - 0s 670us/step - loss: 0.5205 - accuracy: 0.7875 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 86/300\n",
            "160/160 [==============================] - 0s 707us/step - loss: 0.4820 - accuracy: 0.7250 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 87/300\n",
            "160/160 [==============================] - 0s 675us/step - loss: 0.4776 - accuracy: 0.7625 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 88/300\n",
            "160/160 [==============================] - 0s 714us/step - loss: 0.4062 - accuracy: 0.7937 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 89/300\n",
            "160/160 [==============================] - 0s 730us/step - loss: 0.4760 - accuracy: 0.7625 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 90/300\n",
            "160/160 [==============================] - 0s 728us/step - loss: 0.4248 - accuracy: 0.7812 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 91/300\n",
            "160/160 [==============================] - 0s 712us/step - loss: 0.4101 - accuracy: 0.7812 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 92/300\n",
            "160/160 [==============================] - 0s 708us/step - loss: 0.4328 - accuracy: 0.8188 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 93/300\n",
            "160/160 [==============================] - 0s 702us/step - loss: 0.4165 - accuracy: 0.7625 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 94/300\n",
            "160/160 [==============================] - 0s 682us/step - loss: 0.4232 - accuracy: 0.8188 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 95/300\n",
            "160/160 [==============================] - 0s 692us/step - loss: 0.4169 - accuracy: 0.7437 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 96/300\n",
            "160/160 [==============================] - 0s 701us/step - loss: 0.3981 - accuracy: 0.8188 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 97/300\n",
            "160/160 [==============================] - 0s 710us/step - loss: 0.3828 - accuracy: 0.8000 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 98/300\n",
            "160/160 [==============================] - 0s 709us/step - loss: 0.3967 - accuracy: 0.7688 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 99/300\n",
            "160/160 [==============================] - 0s 679us/step - loss: 0.3929 - accuracy: 0.8125 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 100/300\n",
            "160/160 [==============================] - 0s 683us/step - loss: 0.3870 - accuracy: 0.7875 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 101/300\n",
            "160/160 [==============================] - 0s 733us/step - loss: 0.3947 - accuracy: 0.8313 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 102/300\n",
            "160/160 [==============================] - 0s 715us/step - loss: 0.4406 - accuracy: 0.7500 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 103/300\n",
            "160/160 [==============================] - 0s 692us/step - loss: 0.4678 - accuracy: 0.7937 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 104/300\n",
            "160/160 [==============================] - 0s 720us/step - loss: 0.4056 - accuracy: 0.7937 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 105/300\n",
            "160/160 [==============================] - 0s 726us/step - loss: 0.4043 - accuracy: 0.8062 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 106/300\n",
            "160/160 [==============================] - 0s 715us/step - loss: 0.4266 - accuracy: 0.8000 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 107/300\n",
            "160/160 [==============================] - 0s 706us/step - loss: 0.3725 - accuracy: 0.8188 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 108/300\n",
            "160/160 [==============================] - 0s 707us/step - loss: 0.3948 - accuracy: 0.7937 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 109/300\n",
            "160/160 [==============================] - 0s 689us/step - loss: 0.3747 - accuracy: 0.8188 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 110/300\n",
            "160/160 [==============================] - 0s 716us/step - loss: 0.3803 - accuracy: 0.8062 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 111/300\n",
            "160/160 [==============================] - 0s 693us/step - loss: 0.3741 - accuracy: 0.8000 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 112/300\n",
            "160/160 [==============================] - 0s 725us/step - loss: 0.3821 - accuracy: 0.8313 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 113/300\n",
            "160/160 [==============================] - 0s 725us/step - loss: 0.3900 - accuracy: 0.8062 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 114/300\n",
            "160/160 [==============================] - 0s 687us/step - loss: 0.4330 - accuracy: 0.8125 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 115/300\n",
            "160/160 [==============================] - 0s 707us/step - loss: 0.4129 - accuracy: 0.7875 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 116/300\n",
            "160/160 [==============================] - 0s 775us/step - loss: 0.4406 - accuracy: 0.8313 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 117/300\n",
            "160/160 [==============================] - 0s 692us/step - loss: 0.4108 - accuracy: 0.7937 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 118/300\n",
            "160/160 [==============================] - 0s 703us/step - loss: 0.4427 - accuracy: 0.7937 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 119/300\n",
            "160/160 [==============================] - 0s 722us/step - loss: 0.3972 - accuracy: 0.8375 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 120/300\n",
            "160/160 [==============================] - 0s 695us/step - loss: 0.4196 - accuracy: 0.7937 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 121/300\n",
            "160/160 [==============================] - 0s 688us/step - loss: 0.4370 - accuracy: 0.7750 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 122/300\n",
            "160/160 [==============================] - 0s 719us/step - loss: 0.4452 - accuracy: 0.7500 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 123/300\n",
            "160/160 [==============================] - 0s 687us/step - loss: 0.3579 - accuracy: 0.8375 - val_loss: nan - val_accuracy: 0.6500\n",
            "Epoch 124/300\n",
            "160/160 [==============================] - 0s 696us/step - loss: 0.4233 - accuracy: 0.7563 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 125/300\n",
            "160/160 [==============================] - 0s 714us/step - loss: 0.4498 - accuracy: 0.8062 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 126/300\n",
            "160/160 [==============================] - 0s 722us/step - loss: 0.4733 - accuracy: 0.7250 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 127/300\n",
            "160/160 [==============================] - 0s 697us/step - loss: 0.4498 - accuracy: 0.7812 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 128/300\n",
            "160/160 [==============================] - 0s 739us/step - loss: 0.3972 - accuracy: 0.8000 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 129/300\n",
            "160/160 [==============================] - 0s 688us/step - loss: 0.3743 - accuracy: 0.8062 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 130/300\n",
            "160/160 [==============================] - 0s 685us/step - loss: 0.4215 - accuracy: 0.7750 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 131/300\n",
            "160/160 [==============================] - 0s 734us/step - loss: 0.3576 - accuracy: 0.8250 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 132/300\n",
            "160/160 [==============================] - 0s 725us/step - loss: 0.3792 - accuracy: 0.8125 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 133/300\n",
            "160/160 [==============================] - 0s 691us/step - loss: 0.3670 - accuracy: 0.8687 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 134/300\n",
            "160/160 [==============================] - 0s 738us/step - loss: 0.3973 - accuracy: 0.8313 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 135/300\n",
            "160/160 [==============================] - 0s 691us/step - loss: 0.3549 - accuracy: 0.8250 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 136/300\n",
            "160/160 [==============================] - 0s 682us/step - loss: 0.3674 - accuracy: 0.8500 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 137/300\n",
            "160/160 [==============================] - 0s 697us/step - loss: 0.3544 - accuracy: 0.8188 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 138/300\n",
            "160/160 [==============================] - 0s 737us/step - loss: 0.3703 - accuracy: 0.8438 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 139/300\n",
            "160/160 [==============================] - 0s 693us/step - loss: 0.4338 - accuracy: 0.7688 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 140/300\n",
            "160/160 [==============================] - 0s 735us/step - loss: 0.4372 - accuracy: 0.7812 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 141/300\n",
            "160/160 [==============================] - 0s 696us/step - loss: 0.4071 - accuracy: 0.7688 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 142/300\n",
            "160/160 [==============================] - 0s 684us/step - loss: 0.3613 - accuracy: 0.8375 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 143/300\n",
            "160/160 [==============================] - 0s 715us/step - loss: 0.3476 - accuracy: 0.8188 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 144/300\n",
            "160/160 [==============================] - 0s 731us/step - loss: 0.4345 - accuracy: 0.7750 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 145/300\n",
            "160/160 [==============================] - 0s 732us/step - loss: 0.3469 - accuracy: 0.8562 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 146/300\n",
            "160/160 [==============================] - 0s 712us/step - loss: 0.3747 - accuracy: 0.8000 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 147/300\n",
            "160/160 [==============================] - 0s 702us/step - loss: 0.4640 - accuracy: 0.7812 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 148/300\n",
            "160/160 [==============================] - 0s 760us/step - loss: 0.4344 - accuracy: 0.7563 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 149/300\n",
            "160/160 [==============================] - 0s 755us/step - loss: 0.4047 - accuracy: 0.8000 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 150/300\n",
            "160/160 [==============================] - 0s 695us/step - loss: 0.4128 - accuracy: 0.7625 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 151/300\n",
            "160/160 [==============================] - 0s 707us/step - loss: 0.3784 - accuracy: 0.7937 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 152/300\n",
            "160/160 [==============================] - 0s 690us/step - loss: 0.3677 - accuracy: 0.8313 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 153/300\n",
            "160/160 [==============================] - 0s 699us/step - loss: 0.3798 - accuracy: 0.8313 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 154/300\n",
            "160/160 [==============================] - 0s 705us/step - loss: 0.3491 - accuracy: 0.8438 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 155/300\n",
            "160/160 [==============================] - 0s 686us/step - loss: 0.3648 - accuracy: 0.8375 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 156/300\n",
            "160/160 [==============================] - 0s 847us/step - loss: 0.3775 - accuracy: 0.8188 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 157/300\n",
            "160/160 [==============================] - 0s 738us/step - loss: 0.3997 - accuracy: 0.8062 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 158/300\n",
            "160/160 [==============================] - 0s 691us/step - loss: 0.3946 - accuracy: 0.7812 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 159/300\n",
            "160/160 [==============================] - 0s 699us/step - loss: 0.4022 - accuracy: 0.8062 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 160/300\n",
            "160/160 [==============================] - 0s 699us/step - loss: 0.4057 - accuracy: 0.7688 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 161/300\n",
            "160/160 [==============================] - 0s 724us/step - loss: 0.3320 - accuracy: 0.8625 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 162/300\n",
            "160/160 [==============================] - 0s 722us/step - loss: 0.3692 - accuracy: 0.7937 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 163/300\n",
            "160/160 [==============================] - 0s 742us/step - loss: 0.3655 - accuracy: 0.8500 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 164/300\n",
            "160/160 [==============================] - 0s 692us/step - loss: 0.3448 - accuracy: 0.8500 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 165/300\n",
            "160/160 [==============================] - 0s 691us/step - loss: 0.3728 - accuracy: 0.8000 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 166/300\n",
            "160/160 [==============================] - 0s 730us/step - loss: 0.3352 - accuracy: 0.8438 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 167/300\n",
            "160/160 [==============================] - 0s 730us/step - loss: 0.3381 - accuracy: 0.8562 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 168/300\n",
            "160/160 [==============================] - 0s 687us/step - loss: 0.3572 - accuracy: 0.8375 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 169/300\n",
            "160/160 [==============================] - 0s 688us/step - loss: 0.3478 - accuracy: 0.8375 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 170/300\n",
            "160/160 [==============================] - 0s 707us/step - loss: 0.3344 - accuracy: 0.8375 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 171/300\n",
            "160/160 [==============================] - 0s 692us/step - loss: 0.3386 - accuracy: 0.8375 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 172/300\n",
            "160/160 [==============================] - 0s 679us/step - loss: 0.3309 - accuracy: 0.8438 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 173/300\n",
            "160/160 [==============================] - 0s 720us/step - loss: 0.3322 - accuracy: 0.8500 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 174/300\n",
            "160/160 [==============================] - 0s 678us/step - loss: 0.3309 - accuracy: 0.8625 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 175/300\n",
            "160/160 [==============================] - 0s 690us/step - loss: 0.3352 - accuracy: 0.8562 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 176/300\n",
            "160/160 [==============================] - 0s 694us/step - loss: 0.3305 - accuracy: 0.8438 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 177/300\n",
            "160/160 [==============================] - 0s 687us/step - loss: 0.3472 - accuracy: 0.8375 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 178/300\n",
            "160/160 [==============================] - 0s 699us/step - loss: 0.3397 - accuracy: 0.8500 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 179/300\n",
            "160/160 [==============================] - 0s 675us/step - loss: 0.3331 - accuracy: 0.8438 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 180/300\n",
            "160/160 [==============================] - 0s 674us/step - loss: 0.3298 - accuracy: 0.8438 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 181/300\n",
            "160/160 [==============================] - 0s 696us/step - loss: 0.3360 - accuracy: 0.8313 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 182/300\n",
            "160/160 [==============================] - 0s 665us/step - loss: 0.3623 - accuracy: 0.8500 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 183/300\n",
            "160/160 [==============================] - 0s 683us/step - loss: 0.4000 - accuracy: 0.7688 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 184/300\n",
            "160/160 [==============================] - 0s 686us/step - loss: 0.4352 - accuracy: 0.7875 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 185/300\n",
            "160/160 [==============================] - 0s 657us/step - loss: 0.3731 - accuracy: 0.7812 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 186/300\n",
            "160/160 [==============================] - 0s 667us/step - loss: 0.3341 - accuracy: 0.8500 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 187/300\n",
            "160/160 [==============================] - 0s 674us/step - loss: 0.3694 - accuracy: 0.8125 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 188/300\n",
            "160/160 [==============================] - 0s 735us/step - loss: 0.3693 - accuracy: 0.8438 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 189/300\n",
            "160/160 [==============================] - 0s 691us/step - loss: 0.3467 - accuracy: 0.7937 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 190/300\n",
            "160/160 [==============================] - 0s 683us/step - loss: 0.3251 - accuracy: 0.8625 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 191/300\n",
            "160/160 [==============================] - 0s 711us/step - loss: 0.3442 - accuracy: 0.8500 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 192/300\n",
            "160/160 [==============================] - 0s 679us/step - loss: 0.3610 - accuracy: 0.8375 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 193/300\n",
            "160/160 [==============================] - 0s 678us/step - loss: 0.3476 - accuracy: 0.8500 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 194/300\n",
            "160/160 [==============================] - 0s 676us/step - loss: 0.3407 - accuracy: 0.8375 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 195/300\n",
            "160/160 [==============================] - 0s 661us/step - loss: 0.3747 - accuracy: 0.8250 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 196/300\n",
            "160/160 [==============================] - 0s 660us/step - loss: 0.3556 - accuracy: 0.8313 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 197/300\n",
            "160/160 [==============================] - 0s 723us/step - loss: 0.3415 - accuracy: 0.8562 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 198/300\n",
            "160/160 [==============================] - 0s 671us/step - loss: 0.3195 - accuracy: 0.8625 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 199/300\n",
            "160/160 [==============================] - 0s 661us/step - loss: 0.3260 - accuracy: 0.8500 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 200/300\n",
            "160/160 [==============================] - 0s 677us/step - loss: 0.3301 - accuracy: 0.8500 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 201/300\n",
            "160/160 [==============================] - 0s 698us/step - loss: 0.3284 - accuracy: 0.8500 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 202/300\n",
            "160/160 [==============================] - 0s 681us/step - loss: 0.3480 - accuracy: 0.8188 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 203/300\n",
            "160/160 [==============================] - 0s 680us/step - loss: 0.3940 - accuracy: 0.8062 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 204/300\n",
            "160/160 [==============================] - 0s 666us/step - loss: 0.4069 - accuracy: 0.7750 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 205/300\n",
            "160/160 [==============================] - 0s 679us/step - loss: 0.3992 - accuracy: 0.8250 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 206/300\n",
            "160/160 [==============================] - 0s 685us/step - loss: 0.3631 - accuracy: 0.7937 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 207/300\n",
            "160/160 [==============================] - 0s 670us/step - loss: 0.3652 - accuracy: 0.8188 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 208/300\n",
            "160/160 [==============================] - 0s 664us/step - loss: 0.3296 - accuracy: 0.8250 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 209/300\n",
            "160/160 [==============================] - 0s 737us/step - loss: 0.3960 - accuracy: 0.8188 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 210/300\n",
            "160/160 [==============================] - 0s 691us/step - loss: 0.3148 - accuracy: 0.8438 - val_loss: nan - val_accuracy: 0.7750\n",
            "Epoch 211/300\n",
            "160/160 [==============================] - 0s 672us/step - loss: 0.3264 - accuracy: 0.8500 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 212/300\n",
            "160/160 [==============================] - 0s 663us/step - loss: 0.3340 - accuracy: 0.8500 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 213/300\n",
            "160/160 [==============================] - 0s 674us/step - loss: 0.3226 - accuracy: 0.8562 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 214/300\n",
            "160/160 [==============================] - 0s 679us/step - loss: 0.3241 - accuracy: 0.8625 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 215/300\n",
            "160/160 [==============================] - 0s 649us/step - loss: 0.3219 - accuracy: 0.8875 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 216/300\n",
            "160/160 [==============================] - 0s 692us/step - loss: 0.3083 - accuracy: 0.8687 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 217/300\n",
            "160/160 [==============================] - 0s 669us/step - loss: 0.3155 - accuracy: 0.8813 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 218/300\n",
            "160/160 [==============================] - 0s 672us/step - loss: 0.3068 - accuracy: 0.8875 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 219/300\n",
            "160/160 [==============================] - 0s 663us/step - loss: 0.3044 - accuracy: 0.8750 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 220/300\n",
            "160/160 [==============================] - 0s 693us/step - loss: 0.3190 - accuracy: 0.8562 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 221/300\n",
            "160/160 [==============================] - 0s 676us/step - loss: 0.3197 - accuracy: 0.8562 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 222/300\n",
            "160/160 [==============================] - 0s 660us/step - loss: 0.3720 - accuracy: 0.8438 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 223/300\n",
            "160/160 [==============================] - 0s 719us/step - loss: 0.3590 - accuracy: 0.7812 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 224/300\n",
            "160/160 [==============================] - 0s 667us/step - loss: 0.3215 - accuracy: 0.8562 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 225/300\n",
            "160/160 [==============================] - 0s 677us/step - loss: 0.3373 - accuracy: 0.8562 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 226/300\n",
            "160/160 [==============================] - 0s 681us/step - loss: 0.3244 - accuracy: 0.8625 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 227/300\n",
            "160/160 [==============================] - 0s 672us/step - loss: 0.3222 - accuracy: 0.8625 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 228/300\n",
            "160/160 [==============================] - 0s 667us/step - loss: 0.3091 - accuracy: 0.8625 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 229/300\n",
            "160/160 [==============================] - 0s 736us/step - loss: 0.3017 - accuracy: 0.8562 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 230/300\n",
            "160/160 [==============================] - 0s 696us/step - loss: 0.2970 - accuracy: 0.8750 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 231/300\n",
            "160/160 [==============================] - 0s 667us/step - loss: 0.3008 - accuracy: 0.8625 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 232/300\n",
            "160/160 [==============================] - 0s 663us/step - loss: 0.3106 - accuracy: 0.8687 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 233/300\n",
            "160/160 [==============================] - 0s 668us/step - loss: 0.3120 - accuracy: 0.8375 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 234/300\n",
            "160/160 [==============================] - 0s 669us/step - loss: 0.3063 - accuracy: 0.8687 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 235/300\n",
            "160/160 [==============================] - 0s 694us/step - loss: 0.3231 - accuracy: 0.8500 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 236/300\n",
            "160/160 [==============================] - 0s 699us/step - loss: 0.3397 - accuracy: 0.8687 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 237/300\n",
            "160/160 [==============================] - 0s 691us/step - loss: 0.3071 - accuracy: 0.8625 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 238/300\n",
            "160/160 [==============================] - 0s 678us/step - loss: 0.3551 - accuracy: 0.8500 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 239/300\n",
            "160/160 [==============================] - 0s 684us/step - loss: 0.3224 - accuracy: 0.8562 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 240/300\n",
            "160/160 [==============================] - 0s 671us/step - loss: 0.3177 - accuracy: 0.8625 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 241/300\n",
            "160/160 [==============================] - 0s 694us/step - loss: 0.3006 - accuracy: 0.8750 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 242/300\n",
            "160/160 [==============================] - 0s 683us/step - loss: 0.2919 - accuracy: 0.8687 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 243/300\n",
            "160/160 [==============================] - 0s 669us/step - loss: 0.3101 - accuracy: 0.8750 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 244/300\n",
            "160/160 [==============================] - 0s 664us/step - loss: 0.3067 - accuracy: 0.8750 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 245/300\n",
            "160/160 [==============================] - 0s 708us/step - loss: 0.3441 - accuracy: 0.8313 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 246/300\n",
            "160/160 [==============================] - 0s 752us/step - loss: 0.3222 - accuracy: 0.8500 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 247/300\n",
            "160/160 [==============================] - 0s 761us/step - loss: 0.3214 - accuracy: 0.8562 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 248/300\n",
            "160/160 [==============================] - 0s 694us/step - loss: 0.3275 - accuracy: 0.8500 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 249/300\n",
            "160/160 [==============================] - 0s 682us/step - loss: 0.3300 - accuracy: 0.8562 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 250/300\n",
            "160/160 [==============================] - 0s 668us/step - loss: 0.3111 - accuracy: 0.8500 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 251/300\n",
            "160/160 [==============================] - 0s 677us/step - loss: 0.2967 - accuracy: 0.8625 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 252/300\n",
            "160/160 [==============================] - 0s 670us/step - loss: 0.2890 - accuracy: 0.8875 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 253/300\n",
            "160/160 [==============================] - 0s 700us/step - loss: 0.2884 - accuracy: 0.8875 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 254/300\n",
            "160/160 [==============================] - 0s 688us/step - loss: 0.2884 - accuracy: 0.8938 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 255/300\n",
            "160/160 [==============================] - 0s 674us/step - loss: 0.2882 - accuracy: 0.8813 - val_loss: nan - val_accuracy: 0.7750\n",
            "Epoch 256/300\n",
            "160/160 [==============================] - 0s 673us/step - loss: 0.2988 - accuracy: 0.8750 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 257/300\n",
            "160/160 [==============================] - 0s 677us/step - loss: 0.2996 - accuracy: 0.8750 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 258/300\n",
            "160/160 [==============================] - 0s 672us/step - loss: 0.3172 - accuracy: 0.8625 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 259/300\n",
            "160/160 [==============================] - 0s 706us/step - loss: 0.3149 - accuracy: 0.8562 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 260/300\n",
            "160/160 [==============================] - 0s 684us/step - loss: 0.3104 - accuracy: 0.8625 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 261/300\n",
            "160/160 [==============================] - 0s 695us/step - loss: 0.3079 - accuracy: 0.8562 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 262/300\n",
            "160/160 [==============================] - 0s 703us/step - loss: 0.3053 - accuracy: 0.8625 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 263/300\n",
            "160/160 [==============================] - 0s 679us/step - loss: 0.2934 - accuracy: 0.8750 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 264/300\n",
            "160/160 [==============================] - 0s 673us/step - loss: 0.2960 - accuracy: 0.8625 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 265/300\n",
            "160/160 [==============================] - 0s 702us/step - loss: 0.2986 - accuracy: 0.8562 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 266/300\n",
            "160/160 [==============================] - 0s 693us/step - loss: 0.3003 - accuracy: 0.8687 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 267/300\n",
            "160/160 [==============================] - 0s 716us/step - loss: 0.2910 - accuracy: 0.8687 - val_loss: nan - val_accuracy: 0.7750\n",
            "Epoch 268/300\n",
            "160/160 [==============================] - 0s 708us/step - loss: 0.2838 - accuracy: 0.8938 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 269/300\n",
            "160/160 [==============================] - 0s 697us/step - loss: 0.2786 - accuracy: 0.8813 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 270/300\n",
            "160/160 [==============================] - 0s 679us/step - loss: 0.3038 - accuracy: 0.8813 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 271/300\n",
            "160/160 [==============================] - 0s 674us/step - loss: 0.2957 - accuracy: 0.8625 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 272/300\n",
            "160/160 [==============================] - 0s 686us/step - loss: 0.3073 - accuracy: 0.8813 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 273/300\n",
            "160/160 [==============================] - 0s 676us/step - loss: 0.2977 - accuracy: 0.8938 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 274/300\n",
            "160/160 [==============================] - 0s 704us/step - loss: 0.2905 - accuracy: 0.8687 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 275/300\n",
            "160/160 [==============================] - 0s 686us/step - loss: 0.2835 - accuracy: 0.8875 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 276/300\n",
            "160/160 [==============================] - 0s 679us/step - loss: 0.2856 - accuracy: 0.8938 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 277/300\n",
            "160/160 [==============================] - 0s 674us/step - loss: 0.2714 - accuracy: 0.8813 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 278/300\n",
            "160/160 [==============================] - 0s 687us/step - loss: 0.2897 - accuracy: 0.8562 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 279/300\n",
            "160/160 [==============================] - 0s 698us/step - loss: 0.2745 - accuracy: 0.8875 - val_loss: nan - val_accuracy: 0.7750\n",
            "Epoch 280/300\n",
            "160/160 [==============================] - 0s 756us/step - loss: 0.2717 - accuracy: 0.8875 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 281/300\n",
            "160/160 [==============================] - 0s 680us/step - loss: 0.2704 - accuracy: 0.8875 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 282/300\n",
            "160/160 [==============================] - 0s 687us/step - loss: 0.2707 - accuracy: 0.8875 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 283/300\n",
            "160/160 [==============================] - 0s 687us/step - loss: 0.2909 - accuracy: 0.8750 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 284/300\n",
            "160/160 [==============================] - 0s 679us/step - loss: 0.3077 - accuracy: 0.8562 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 285/300\n",
            "160/160 [==============================] - 0s 674us/step - loss: 0.2999 - accuracy: 0.8625 - val_loss: nan - val_accuracy: 0.6750\n",
            "Epoch 286/300\n",
            "160/160 [==============================] - 0s 697us/step - loss: 0.3215 - accuracy: 0.8562 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 287/300\n",
            "160/160 [==============================] - 0s 714us/step - loss: 0.2905 - accuracy: 0.8562 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 288/300\n",
            "160/160 [==============================] - 0s 682us/step - loss: 0.2912 - accuracy: 0.8750 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 289/300\n",
            "160/160 [==============================] - 0s 692us/step - loss: 0.2908 - accuracy: 0.8750 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 290/300\n",
            "160/160 [==============================] - 0s 678us/step - loss: 0.2727 - accuracy: 0.9062 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 291/300\n",
            "160/160 [==============================] - 0s 685us/step - loss: 0.2653 - accuracy: 0.8875 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 292/300\n",
            "160/160 [==============================] - 0s 704us/step - loss: 0.2687 - accuracy: 0.9000 - val_loss: nan - val_accuracy: 0.7250\n",
            "Epoch 293/300\n",
            "160/160 [==============================] - 0s 705us/step - loss: 0.2630 - accuracy: 0.8875 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 294/300\n",
            "160/160 [==============================] - 0s 686us/step - loss: 0.2647 - accuracy: 0.9062 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 295/300\n",
            "160/160 [==============================] - 0s 684us/step - loss: 0.3112 - accuracy: 0.8500 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 296/300\n",
            "160/160 [==============================] - 0s 674us/step - loss: 0.2706 - accuracy: 0.8750 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 297/300\n",
            "160/160 [==============================] - 0s 685us/step - loss: 0.2769 - accuracy: 0.9000 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 298/300\n",
            "160/160 [==============================] - 0s 700us/step - loss: 0.2809 - accuracy: 0.8687 - val_loss: nan - val_accuracy: 0.7000\n",
            "Epoch 299/300\n",
            "160/160 [==============================] - 0s 696us/step - loss: 0.2810 - accuracy: 0.8750 - val_loss: nan - val_accuracy: 0.7500\n",
            "Epoch 300/300\n",
            "160/160 [==============================] - 0s 671us/step - loss: 0.2721 - accuracy: 0.8938 - val_loss: nan - val_accuracy: 0.7250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "d43023d26a87e3c4702b96bea5962c990c76aa0a",
        "id": "Wl7-odnFPIvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test, batch_size=80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "20cfe7f5891e3c26c599fa4cd728ac0a499ac70e",
        "id": "9E0V7SSAPIvR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "83d07eba-1319-4bb0-c6df-6139b06e35a0"
      },
      "source": [
        "print(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1)))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.85      0.76        20\n",
            "           1       0.80      0.60      0.69        20\n",
            "\n",
            "    accuracy                           0.73        40\n",
            "   macro avg       0.74      0.72      0.72        40\n",
            "weighted avg       0.74      0.72      0.72        40\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "017f2431a45766fb0d4b2ef17ce613c1142ca085",
        "id": "dc0I9FBYPIvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(\"ranking-based average precision : {:.3f}\".format(label_ranking_average_precision_score(y_test.todense(), y_pred)))\n",
        "# print(\"Ranking loss : {:.3f}\".format(label_ranking_loss(y_test.todense(), y_pred)))\n",
        "# print(\"Coverage_error : {:.3f}\".format(coverage_error(y_test.todense(), y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "0fcf027f00a7031dd7dc7d25c0c6ff362c39954b",
        "id": "gnFeRvhZPIvZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "65866c0d-e87e-4c90-f628-38b19fb86ef4"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plt.figure(figsize=(5, 5))\n",
        "plot_confusion_matrix(cnf_matrix, classes=['0', '1'],\n",
        "                      title='Confusion matrix, without normalization')\n",
        "plt.show()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWIAAAFgCAYAAACBlHNxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlmYo1uT2noY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}